{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        print(\"Mask shape (in attention): \", mask.shape)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        print(\"Att. scores shape:\", attn_scores.shape)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        print(\"In combined Heads:\", x.shape)\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        print(\"Query shape before\", Q.shape)\n",
    "        print(\"Key shape before\", K.shape)\n",
    "        print(\"Value shape before\", V.shape)\n",
    "\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        print(\"Query shape after\", Q.shape)\n",
    "        print(\"Key shape after\", K.shape)\n",
    "        print(\"Value shape after\", V.shape)\n",
    "        print(\"------------ LAYER FINISHED --------------\")\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        print(src_mask.shape)\n",
    "        print(tgt_mask.shape)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++ ENCODER FINISHED +++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 25])\n",
      "torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 10000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "d_ff = 2048\n",
    "max_seq_length_src = 25\n",
    "max_seq_length_tgt = 50\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, 2*max_seq_length_src, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (4, max_seq_length_src))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (4, max_seq_length_tgt))  # (batch_size, seq_length)\n",
    "print(src_data.shape)\n",
    "print(tgt_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 1, 25])\n",
      "torch.Size([4, 1, 50, 50])\n",
      "Query shape before torch.Size([4, 25, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 25, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 25, 25])\n",
      "In combined Heads: torch.Size([4, 8, 25, 64])\n",
      "Query shape before torch.Size([4, 25, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 25, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 25, 25])\n",
      "In combined Heads: torch.Size([4, 8, 25, 64])\n",
      "Query shape before torch.Size([4, 25, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 25, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 25, 25])\n",
      "In combined Heads: torch.Size([4, 8, 25, 64])\n",
      "Query shape before torch.Size([4, 25, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 25, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 25, 25])\n",
      "In combined Heads: torch.Size([4, 8, 25, 64])\n",
      "+++++++++++++++++++++++++++++++++++++++++ ENCODER FINISHED +++++++++++++++++++++++++++++++++++++++++++++\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 50, 512])\n",
      "Value shape before torch.Size([4, 50, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 50, 64])\n",
      "Value shape after torch.Size([4, 8, 50, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 50, 50])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 50])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 25])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 50, 512])\n",
      "Value shape before torch.Size([4, 50, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 50, 64])\n",
      "Value shape after torch.Size([4, 8, 50, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 50, 50])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 50])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 25])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 50, 512])\n",
      "Value shape before torch.Size([4, 50, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 50, 64])\n",
      "Value shape after torch.Size([4, 8, 50, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 50, 50])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 50])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 25])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 50, 512])\n",
      "Value shape before torch.Size([4, 50, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 50, 64])\n",
      "Value shape after torch.Size([4, 8, 50, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 50, 50])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 50])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "Query shape before torch.Size([4, 50, 512])\n",
      "Key shape before torch.Size([4, 25, 512])\n",
      "Value shape before torch.Size([4, 25, 512])\n",
      "Query shape after torch.Size([4, 8, 50, 64])\n",
      "Key shape after torch.Size([4, 8, 25, 64])\n",
      "Value shape after torch.Size([4, 8, 25, 64])\n",
      "------------ LAYER FINISHED --------------\n",
      "Mask shape (in attention):  torch.Size([4, 1, 1, 25])\n",
      "Att. scores shape: torch.Size([4, 8, 50, 25])\n",
      "In combined Heads: torch.Size([4, 8, 50, 64])\n",
      "torch.Size([4, 50, 10000])\n",
      "torch.Size([200, 10000])\n",
      "torch.Size([200])\n",
      "Epoch: 1, Loss: 9.392040252685547\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data)\n",
    "    print(output.shape)\n",
    "    print(output.contiguous().view(-1, tgt_vocab_size).shape)\n",
    "    print(tgt_data.contiguous().view(-1).shape)\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/p/c80afbc9ffb1/\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(self,num_tokens,dim_model,num_heads,num_encoder_layers,num_decoder_layers,dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(dim_model=dim_model, dropout_p=dropout_p, max_len=5000)\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(d_model=dim_model,nhead=num_heads,num_encoder_layers=num_encoder_layers,num_decoder_layers=num_decoder_layers,dropout=dropout_p)\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "\n",
    "    def forward(self,src,tgt,tgt_mask):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src)*math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt)*math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        # we permute to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt, src_mask=None, tgt_mask=tgt_mask)\n",
    "        out = self.out(transformer_out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trainable parameters of the model are:  60712762\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(num_tokens=30522, dim_model=512, num_heads=8, num_encoder_layers=4, num_decoder_layers=4, dropout_p=0.1).to(device)\n",
    "opt = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "print(\"The trainable parameters of the model are: \", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "path_train_a = f'C:/Users/admitos/Desktop/ThesisUU/Phase_2/data_phase2/Combined/train_caps_1.pkl'\n",
    "path_val_a = f'C:/Users/admitos/Desktop/ThesisUU/Phase_2/data_phase2/Combined/val_caps_1.pkl'\n",
    "path_test_a = f'C:/Users/admitos/Desktop/ThesisUU/Phase_2/data_phase2/Combined/test_caps_1.pkl'\n",
    "\n",
    "with open(path_train_a, 'rb') as f:\n",
    "    tokenized_train_captions1 = pickle.load(f)\n",
    "\n",
    "with open(path_val_a, 'rb') as f:\n",
    "    tokenized_val_captions1 = pickle.load(f)\n",
    "    \n",
    "with open(path_test_a, 'rb') as f:\n",
    "    tokenized_test_captions1 = pickle.load(f)\n",
    "\n",
    "path_train_stories = f'C:/Users/admitos/Desktop/ThesisUU/Phase_2/data_phase2/Combined/train_stories.pkl'\n",
    "path_val_stories = f'C:/Users/admitos/Desktop/ThesisUU/Phase_2/data_phase2/Combined/val_stories.pkl'\n",
    "path_test_stories = f'C:/Users/admitos/Desktop/ThesisUU/Phase_2/data_phase2/Combined/test_stories.pkl'\n",
    "\n",
    "with open(path_train_stories, 'rb') as f:\n",
    "    tokenized_train_stories = pickle.load(f)\n",
    "\n",
    "with open(path_val_stories, 'rb') as f:\n",
    "    tokenized_val_stories = pickle.load(f)\n",
    "\n",
    "with open(path_test_stories, 'rb') as f:\n",
    "    tokenized_test_stories = pickle.load(f)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, entries, references):\n",
    "        self.entries = entries\n",
    "        self.references = references\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        item['caption'] = self.entries[idx]\n",
    "        item['story'] = self.references [idx] \n",
    "        return item\n",
    "\n",
    "        \n",
    "def my_collate_fn(batch):\n",
    "    input_ids = [item['caption'].transpose(0,1) for item in batch]\n",
    "    target_ids = [item['story'].transpose(0,1) for item in batch]\n",
    "    \n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    target_ids_padded = pad_sequence(target_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    final_input_ids = [item.transpose(0,1) for item in input_ids_padded]\n",
    "    final_labels = [item.transpose(0,1) for item in target_ids_padded]\n",
    "\n",
    "    return {'caption_ids': final_input_ids, 'story_ids': final_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s = 10\n",
    "train_dataset1 = CustomDataset(tokenized_train_captions1, tokenized_train_stories)\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=b_s, shuffle=False, collate_fn=my_collate_fn)\n",
    "\n",
    "## Prepareing Validation Loaders \n",
    "val_dataset1 = CustomDataset(tokenized_val_captions1, tokenized_val_stories)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=b_s, shuffle=False, collate_fn=my_collate_fn)\n",
    "\n",
    "## Prepareing Test Loaders \n",
    "test_dataset1 = CustomDataset(tokenized_test_captions1, tokenized_test_stories)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=b_s, shuffle=False, collate_fn=my_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions shape: torch.Size([1, 79]) torch.Size([1, 79])\n",
      "Stories shape:  torch.Size([1, 81]) torch.Size([1, 81])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader1:\n",
    "    print(\"Captions shape:\", batch['caption_ids'][0].shape, batch['caption_ids'][1].shape)\n",
    "    print(\"Stories shape: \", batch['story_ids'][0].shape, batch['story_ids'][0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_before_model(batch):\n",
    "    batch_caption_ids = torch.stack(batch['caption_ids']).squeeze(1)\n",
    "    batch_story_ids = torch.stack(batch['story_ids']).squeeze(1)\n",
    "    return batch_caption_ids, batch_story_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trainable parameters of the model are:  60712762\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "print(\"The trainable parameters of the model are: \", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx,batch in enumerate(dataloader):\n",
    "        caps, stories = transform_before_model(batch)\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = stories[:,:-1]\n",
    "        y_expected = stories[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(caps, y_input, tgt_mask)\n",
    "        #pred = model(caps, y_input)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "        print(f'Iteration: {idx}/{len(dataloader)}, with running loss: {loss.item()}')\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            caps, stories = transform_before_model(batch)\n",
    "        \n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = stories[:,:-1]\n",
    "            y_expected = stories[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(caps, y_input, tgt_mask)\n",
    "            #pred = model(caps, y_input)\n",
    "            \n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            print(f'Iteration: {idx}/{len(dataloader)}, with running loss: {loss.item()}')\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      "Iteration: 0/2657, with running loss: 3.435696601867676\n",
      "Iteration: 1/2657, with running loss: 3.595099925994873\n",
      "Iteration: 2/2657, with running loss: 3.0748696327209473\n",
      "Iteration: 3/2657, with running loss: 3.4868874549865723\n",
      "Iteration: 4/2657, with running loss: 3.357516288757324\n",
      "Iteration: 5/2657, with running loss: 2.9999752044677734\n",
      "Iteration: 6/2657, with running loss: 3.655576467514038\n",
      "Iteration: 7/2657, with running loss: 3.0454952716827393\n",
      "Iteration: 8/2657, with running loss: 2.570434808731079\n",
      "Iteration: 9/2657, with running loss: 2.8568203449249268\n",
      "Iteration: 10/2657, with running loss: 3.0220389366149902\n",
      "Iteration: 11/2657, with running loss: 2.7257168292999268\n",
      "Iteration: 12/2657, with running loss: 3.416012763977051\n",
      "Iteration: 13/2657, with running loss: 3.6815953254699707\n",
      "Iteration: 14/2657, with running loss: 3.2250218391418457\n",
      "Iteration: 15/2657, with running loss: 2.964179515838623\n",
      "Iteration: 16/2657, with running loss: 3.687410593032837\n",
      "Iteration: 17/2657, with running loss: 2.5017967224121094\n",
      "Iteration: 18/2657, with running loss: 3.2445662021636963\n",
      "Iteration: 19/2657, with running loss: 3.2943453788757324\n",
      "Iteration: 20/2657, with running loss: 3.8332114219665527\n",
      "Iteration: 21/2657, with running loss: 3.987414836883545\n",
      "Iteration: 22/2657, with running loss: 2.9670376777648926\n",
      "Iteration: 23/2657, with running loss: 1.9340451955795288\n",
      "Iteration: 24/2657, with running loss: 3.7299721240997314\n",
      "Iteration: 25/2657, with running loss: 3.1252386569976807\n",
      "Iteration: 26/2657, with running loss: 2.9862170219421387\n",
      "Iteration: 27/2657, with running loss: 1.9724342823028564\n",
      "Iteration: 28/2657, with running loss: 3.759910821914673\n",
      "Iteration: 29/2657, with running loss: 2.2801642417907715\n",
      "Iteration: 30/2657, with running loss: 2.1053526401519775\n",
      "Iteration: 31/2657, with running loss: 3.271069049835205\n",
      "Iteration: 32/2657, with running loss: 2.9502506256103516\n",
      "Iteration: 33/2657, with running loss: 2.6102054119110107\n",
      "Iteration: 34/2657, with running loss: 2.7677299976348877\n",
      "Iteration: 35/2657, with running loss: 3.1811676025390625\n",
      "Iteration: 36/2657, with running loss: 3.0261623859405518\n",
      "Iteration: 37/2657, with running loss: 3.250380754470825\n",
      "Iteration: 38/2657, with running loss: 3.4597344398498535\n",
      "Iteration: 39/2657, with running loss: 3.98888897895813\n",
      "Iteration: 40/2657, with running loss: 2.4234182834625244\n",
      "Iteration: 41/2657, with running loss: 3.259153127670288\n",
      "Iteration: 42/2657, with running loss: 3.2815377712249756\n",
      "Iteration: 43/2657, with running loss: 2.7428555488586426\n",
      "Iteration: 44/2657, with running loss: 3.4864466190338135\n",
      "Iteration: 45/2657, with running loss: 3.302910566329956\n",
      "Iteration: 46/2657, with running loss: 3.1789257526397705\n",
      "Iteration: 47/2657, with running loss: 3.874223470687866\n",
      "Iteration: 48/2657, with running loss: 2.961055278778076\n",
      "Iteration: 49/2657, with running loss: 2.8415679931640625\n",
      "Iteration: 50/2657, with running loss: 3.4823195934295654\n",
      "Iteration: 51/2657, with running loss: 2.752751350402832\n",
      "Iteration: 52/2657, with running loss: 3.0112037658691406\n",
      "Iteration: 53/2657, with running loss: 2.8894612789154053\n",
      "Iteration: 54/2657, with running loss: 2.678889274597168\n",
      "Iteration: 55/2657, with running loss: 3.812551736831665\n",
      "Iteration: 56/2657, with running loss: 3.075002670288086\n",
      "Iteration: 57/2657, with running loss: 3.2639424800872803\n",
      "Iteration: 58/2657, with running loss: 2.654672384262085\n",
      "Iteration: 59/2657, with running loss: 2.7973477840423584\n",
      "Iteration: 60/2657, with running loss: 2.8076512813568115\n",
      "Iteration: 61/2657, with running loss: 3.192471504211426\n",
      "Iteration: 62/2657, with running loss: 2.499303102493286\n",
      "Iteration: 63/2657, with running loss: 2.9084105491638184\n",
      "Iteration: 64/2657, with running loss: 3.383669853210449\n",
      "Iteration: 65/2657, with running loss: 3.5637404918670654\n",
      "Iteration: 66/2657, with running loss: 3.066955327987671\n",
      "Iteration: 67/2657, with running loss: 3.451610803604126\n",
      "Iteration: 68/2657, with running loss: 3.3651912212371826\n",
      "Iteration: 69/2657, with running loss: 2.229428291320801\n",
      "Iteration: 70/2657, with running loss: 3.27239990234375\n",
      "Iteration: 71/2657, with running loss: 3.121472120285034\n",
      "Iteration: 72/2657, with running loss: 3.268414258956909\n",
      "Iteration: 73/2657, with running loss: 2.9085943698883057\n",
      "Iteration: 74/2657, with running loss: 3.753335475921631\n",
      "Iteration: 75/2657, with running loss: 3.1484036445617676\n",
      "Iteration: 76/2657, with running loss: 3.5837435722351074\n",
      "Iteration: 77/2657, with running loss: 3.412982702255249\n",
      "Iteration: 78/2657, with running loss: 3.110227108001709\n",
      "Iteration: 79/2657, with running loss: 3.23919939994812\n",
      "Iteration: 80/2657, with running loss: 2.585752487182617\n",
      "Iteration: 81/2657, with running loss: 3.485170841217041\n",
      "Iteration: 82/2657, with running loss: 3.479640007019043\n",
      "Iteration: 83/2657, with running loss: 2.4966065883636475\n",
      "Iteration: 84/2657, with running loss: 2.8377108573913574\n",
      "Iteration: 85/2657, with running loss: 3.4243645668029785\n",
      "Iteration: 86/2657, with running loss: 2.662463665008545\n",
      "Iteration: 87/2657, with running loss: 3.656322956085205\n",
      "Iteration: 88/2657, with running loss: 3.0530622005462646\n",
      "Iteration: 89/2657, with running loss: 2.7635364532470703\n",
      "Iteration: 90/2657, with running loss: 2.4273934364318848\n",
      "Iteration: 91/2657, with running loss: 3.085186243057251\n",
      "Iteration: 92/2657, with running loss: 3.7021474838256836\n",
      "Iteration: 93/2657, with running loss: 3.853388547897339\n",
      "Iteration: 94/2657, with running loss: 3.015026569366455\n",
      "Iteration: 95/2657, with running loss: 2.9779775142669678\n",
      "Iteration: 96/2657, with running loss: 3.279794454574585\n",
      "Iteration: 97/2657, with running loss: 3.4454519748687744\n",
      "Iteration: 98/2657, with running loss: 2.700450897216797\n",
      "Iteration: 99/2657, with running loss: 3.161517858505249\n",
      "Iteration: 100/2657, with running loss: 3.7049880027770996\n",
      "Iteration: 101/2657, with running loss: 2.6170012950897217\n",
      "Iteration: 102/2657, with running loss: 2.7234373092651367\n",
      "Iteration: 103/2657, with running loss: 3.9388201236724854\n",
      "Iteration: 104/2657, with running loss: 3.6032421588897705\n",
      "Iteration: 105/2657, with running loss: 3.6184403896331787\n",
      "Iteration: 106/2657, with running loss: 3.776024103164673\n",
      "Iteration: 107/2657, with running loss: 2.496948719024658\n",
      "Iteration: 108/2657, with running loss: 3.4570674896240234\n",
      "Iteration: 109/2657, with running loss: 3.2286055088043213\n",
      "Iteration: 110/2657, with running loss: 3.3798277378082275\n",
      "Iteration: 111/2657, with running loss: 3.0136985778808594\n",
      "Iteration: 112/2657, with running loss: 2.4782063961029053\n",
      "Iteration: 113/2657, with running loss: 3.3033759593963623\n",
      "Iteration: 114/2657, with running loss: 3.1556334495544434\n",
      "Iteration: 115/2657, with running loss: 3.6499040126800537\n",
      "Iteration: 116/2657, with running loss: 2.831200361251831\n",
      "Iteration: 117/2657, with running loss: 3.3772900104522705\n",
      "Iteration: 118/2657, with running loss: 3.200575590133667\n",
      "Iteration: 119/2657, with running loss: 3.7540194988250732\n",
      "Iteration: 120/2657, with running loss: 3.092200517654419\n",
      "Iteration: 121/2657, with running loss: 3.60248064994812\n",
      "Iteration: 122/2657, with running loss: 3.05397367477417\n",
      "Iteration: 123/2657, with running loss: 3.31048321723938\n",
      "Iteration: 124/2657, with running loss: 2.9985592365264893\n",
      "Iteration: 125/2657, with running loss: 2.5805792808532715\n",
      "Iteration: 126/2657, with running loss: 2.93343448638916\n",
      "Iteration: 127/2657, with running loss: 3.0231542587280273\n",
      "Iteration: 128/2657, with running loss: 3.334625244140625\n",
      "Iteration: 129/2657, with running loss: 3.4045095443725586\n",
      "Iteration: 130/2657, with running loss: 2.96915864944458\n",
      "Iteration: 131/2657, with running loss: 3.224724769592285\n",
      "Iteration: 132/2657, with running loss: 3.5235109329223633\n",
      "Iteration: 133/2657, with running loss: 2.4882454872131348\n",
      "Iteration: 134/2657, with running loss: 3.073782444000244\n",
      "Iteration: 135/2657, with running loss: 2.7109134197235107\n",
      "Iteration: 136/2657, with running loss: 2.499863862991333\n",
      "Iteration: 137/2657, with running loss: 2.844648838043213\n",
      "Iteration: 138/2657, with running loss: 3.629760265350342\n",
      "Iteration: 139/2657, with running loss: 3.792065382003784\n",
      "Iteration: 140/2657, with running loss: 3.5150630474090576\n",
      "Iteration: 141/2657, with running loss: 1.3898123502731323\n",
      "Iteration: 142/2657, with running loss: 2.7037761211395264\n",
      "Iteration: 143/2657, with running loss: 3.5159153938293457\n",
      "Iteration: 144/2657, with running loss: 2.455925703048706\n",
      "Iteration: 145/2657, with running loss: 2.1053593158721924\n",
      "Iteration: 146/2657, with running loss: 3.731557607650757\n",
      "Iteration: 147/2657, with running loss: 2.6155741214752197\n",
      "Iteration: 148/2657, with running loss: 3.7899932861328125\n",
      "Iteration: 149/2657, with running loss: 3.3307502269744873\n",
      "Iteration: 150/2657, with running loss: 2.249791383743286\n",
      "Iteration: 151/2657, with running loss: 3.100741147994995\n",
      "Iteration: 152/2657, with running loss: 3.027806282043457\n",
      "Iteration: 153/2657, with running loss: 3.6837949752807617\n",
      "Iteration: 154/2657, with running loss: 3.292041778564453\n",
      "Iteration: 155/2657, with running loss: 3.503089427947998\n",
      "Iteration: 156/2657, with running loss: 3.6461687088012695\n",
      "Iteration: 157/2657, with running loss: 3.5727901458740234\n",
      "Iteration: 158/2657, with running loss: 3.9077694416046143\n",
      "Iteration: 159/2657, with running loss: 3.4500234127044678\n",
      "Iteration: 160/2657, with running loss: 3.410614013671875\n",
      "Iteration: 161/2657, with running loss: 3.016861915588379\n",
      "Iteration: 162/2657, with running loss: 2.6920132637023926\n",
      "Iteration: 163/2657, with running loss: 1.9866524934768677\n",
      "Iteration: 164/2657, with running loss: 2.454362154006958\n",
      "Iteration: 165/2657, with running loss: 3.296978712081909\n",
      "Iteration: 166/2657, with running loss: 2.5785040855407715\n",
      "Iteration: 167/2657, with running loss: 3.481926679611206\n",
      "Iteration: 168/2657, with running loss: 3.1946282386779785\n",
      "Iteration: 169/2657, with running loss: 3.8649704456329346\n",
      "Iteration: 170/2657, with running loss: 2.942410945892334\n",
      "Iteration: 171/2657, with running loss: 2.712836265563965\n",
      "Iteration: 172/2657, with running loss: 3.5543136596679688\n",
      "Iteration: 173/2657, with running loss: 2.988802671432495\n",
      "Iteration: 174/2657, with running loss: 2.3069417476654053\n",
      "Iteration: 175/2657, with running loss: 2.861518383026123\n",
      "Iteration: 176/2657, with running loss: 4.3476104736328125\n",
      "Iteration: 177/2657, with running loss: 3.3139231204986572\n",
      "Iteration: 178/2657, with running loss: 3.119669198989868\n",
      "Iteration: 179/2657, with running loss: 3.050442695617676\n",
      "Iteration: 180/2657, with running loss: 3.0504846572875977\n",
      "Iteration: 181/2657, with running loss: 2.9849367141723633\n",
      "Iteration: 182/2657, with running loss: 3.185572862625122\n",
      "Iteration: 183/2657, with running loss: 3.675083875656128\n",
      "Iteration: 184/2657, with running loss: 2.8065805435180664\n",
      "Iteration: 185/2657, with running loss: 4.073877811431885\n",
      "Iteration: 186/2657, with running loss: 3.5873193740844727\n",
      "Iteration: 187/2657, with running loss: 3.419512987136841\n",
      "Iteration: 188/2657, with running loss: 3.0926618576049805\n",
      "Iteration: 189/2657, with running loss: 3.761404275894165\n",
      "Iteration: 190/2657, with running loss: 2.9403223991394043\n",
      "Iteration: 191/2657, with running loss: 2.8482117652893066\n",
      "Iteration: 192/2657, with running loss: 2.669567108154297\n",
      "Iteration: 193/2657, with running loss: 2.8091979026794434\n",
      "Iteration: 194/2657, with running loss: 2.9183428287506104\n",
      "Iteration: 195/2657, with running loss: 3.429938316345215\n",
      "Iteration: 196/2657, with running loss: 3.091305732727051\n",
      "Iteration: 197/2657, with running loss: 3.408484935760498\n",
      "Iteration: 198/2657, with running loss: 2.891335964202881\n",
      "Iteration: 199/2657, with running loss: 2.9432425498962402\n",
      "Iteration: 200/2657, with running loss: 3.0793607234954834\n",
      "Iteration: 201/2657, with running loss: 2.6007144451141357\n",
      "Iteration: 202/2657, with running loss: 3.162583351135254\n",
      "Iteration: 203/2657, with running loss: 3.5582826137542725\n",
      "Iteration: 204/2657, with running loss: 4.0330119132995605\n",
      "Iteration: 205/2657, with running loss: 3.800178050994873\n",
      "Iteration: 206/2657, with running loss: 2.9650278091430664\n",
      "Iteration: 207/2657, with running loss: 4.016156196594238\n",
      "Iteration: 208/2657, with running loss: 3.7101235389709473\n",
      "Iteration: 209/2657, with running loss: 2.778424024581909\n",
      "Iteration: 210/2657, with running loss: 3.096395969390869\n",
      "Iteration: 211/2657, with running loss: 2.263706684112549\n",
      "Iteration: 212/2657, with running loss: 3.2213897705078125\n",
      "Iteration: 213/2657, with running loss: 2.672706127166748\n",
      "Iteration: 214/2657, with running loss: 2.8023276329040527\n",
      "Iteration: 215/2657, with running loss: 3.369284152984619\n",
      "Iteration: 216/2657, with running loss: 2.5116400718688965\n",
      "Iteration: 217/2657, with running loss: 2.913416862487793\n",
      "Iteration: 218/2657, with running loss: 3.1951937675476074\n",
      "Iteration: 219/2657, with running loss: 3.340365171432495\n",
      "Iteration: 220/2657, with running loss: 3.6177000999450684\n",
      "Iteration: 221/2657, with running loss: 3.0191922187805176\n",
      "Iteration: 222/2657, with running loss: 2.605006456375122\n",
      "Iteration: 223/2657, with running loss: 3.6264278888702393\n",
      "Iteration: 224/2657, with running loss: 2.7278237342834473\n",
      "Iteration: 225/2657, with running loss: 3.1266109943389893\n",
      "Iteration: 226/2657, with running loss: 2.8310298919677734\n",
      "Iteration: 227/2657, with running loss: 2.724571466445923\n",
      "Iteration: 228/2657, with running loss: 3.074162244796753\n",
      "Iteration: 229/2657, with running loss: 2.9436185359954834\n",
      "Iteration: 230/2657, with running loss: 3.1015992164611816\n",
      "Iteration: 231/2657, with running loss: 3.3855133056640625\n",
      "Iteration: 232/2657, with running loss: 2.994743585586548\n",
      "Iteration: 233/2657, with running loss: 3.297778606414795\n",
      "Iteration: 234/2657, with running loss: 2.8323216438293457\n",
      "Iteration: 235/2657, with running loss: 2.773568868637085\n",
      "Iteration: 236/2657, with running loss: 3.213728904724121\n",
      "Iteration: 237/2657, with running loss: 3.3719210624694824\n",
      "Iteration: 238/2657, with running loss: 2.881957769393921\n",
      "Iteration: 239/2657, with running loss: 3.19767165184021\n",
      "Iteration: 240/2657, with running loss: 3.306096076965332\n",
      "Iteration: 241/2657, with running loss: 3.0209338665008545\n",
      "Iteration: 242/2657, with running loss: 2.8545303344726562\n",
      "Iteration: 243/2657, with running loss: 3.711620569229126\n",
      "Iteration: 244/2657, with running loss: 2.8586606979370117\n",
      "Iteration: 245/2657, with running loss: 2.652958869934082\n",
      "Iteration: 246/2657, with running loss: 3.2062833309173584\n",
      "Iteration: 247/2657, with running loss: 3.247225046157837\n",
      "Iteration: 248/2657, with running loss: 3.2669382095336914\n",
      "Iteration: 249/2657, with running loss: 2.1150033473968506\n",
      "Iteration: 250/2657, with running loss: 3.722367286682129\n",
      "Iteration: 251/2657, with running loss: 3.3825387954711914\n",
      "Iteration: 252/2657, with running loss: 3.397524356842041\n",
      "Iteration: 253/2657, with running loss: 3.383488655090332\n",
      "Iteration: 254/2657, with running loss: 3.2317166328430176\n",
      "Iteration: 255/2657, with running loss: 3.308690309524536\n",
      "Iteration: 256/2657, with running loss: 2.8483715057373047\n",
      "Iteration: 257/2657, with running loss: 2.91811203956604\n",
      "Iteration: 258/2657, with running loss: 3.6735286712646484\n",
      "Iteration: 259/2657, with running loss: 2.978982448577881\n",
      "Iteration: 260/2657, with running loss: 2.8131535053253174\n",
      "Iteration: 261/2657, with running loss: 2.9376821517944336\n",
      "Iteration: 262/2657, with running loss: 3.1788392066955566\n",
      "Iteration: 263/2657, with running loss: 2.4607834815979004\n",
      "Iteration: 264/2657, with running loss: 3.4353413581848145\n",
      "Iteration: 265/2657, with running loss: 3.109736442565918\n",
      "Iteration: 266/2657, with running loss: 3.6762986183166504\n",
      "Iteration: 267/2657, with running loss: 2.621933937072754\n",
      "Iteration: 268/2657, with running loss: 3.3478307723999023\n",
      "Iteration: 269/2657, with running loss: 3.756936550140381\n",
      "Iteration: 270/2657, with running loss: 2.873479127883911\n",
      "Iteration: 271/2657, with running loss: 2.5016326904296875\n",
      "Iteration: 272/2657, with running loss: 3.5590076446533203\n",
      "Iteration: 273/2657, with running loss: 3.8185372352600098\n",
      "Iteration: 274/2657, with running loss: 3.441537618637085\n",
      "Iteration: 275/2657, with running loss: 3.428072690963745\n",
      "Iteration: 276/2657, with running loss: 3.4326171875\n",
      "Iteration: 277/2657, with running loss: 2.4665539264678955\n",
      "Iteration: 278/2657, with running loss: 2.1475164890289307\n",
      "Iteration: 279/2657, with running loss: 3.0904927253723145\n",
      "Iteration: 280/2657, with running loss: 2.6953418254852295\n",
      "Iteration: 281/2657, with running loss: 2.921365261077881\n",
      "Iteration: 282/2657, with running loss: 2.7590510845184326\n",
      "Iteration: 283/2657, with running loss: 3.4951579570770264\n",
      "Iteration: 284/2657, with running loss: 2.251042604446411\n",
      "Iteration: 285/2657, with running loss: 3.194370746612549\n",
      "Iteration: 286/2657, with running loss: 3.166640043258667\n",
      "Iteration: 287/2657, with running loss: 2.6739678382873535\n",
      "Iteration: 288/2657, with running loss: 3.382310390472412\n",
      "Iteration: 289/2657, with running loss: 3.354388952255249\n",
      "Iteration: 290/2657, with running loss: 1.771283507347107\n",
      "Iteration: 291/2657, with running loss: 3.292577028274536\n",
      "Iteration: 292/2657, with running loss: 2.955317974090576\n",
      "Iteration: 293/2657, with running loss: 2.671727180480957\n",
      "Iteration: 294/2657, with running loss: 3.1142544746398926\n",
      "Iteration: 295/2657, with running loss: 4.035815715789795\n",
      "Iteration: 296/2657, with running loss: 3.176223039627075\n",
      "Iteration: 297/2657, with running loss: 2.407397985458374\n",
      "Iteration: 298/2657, with running loss: 2.904205322265625\n",
      "Iteration: 299/2657, with running loss: 3.1463325023651123\n",
      "Iteration: 300/2657, with running loss: 2.597823143005371\n",
      "Iteration: 301/2657, with running loss: 3.1232056617736816\n",
      "Iteration: 302/2657, with running loss: 3.355415105819702\n",
      "Iteration: 303/2657, with running loss: 2.444559097290039\n",
      "Iteration: 304/2657, with running loss: 3.514145851135254\n",
      "Iteration: 305/2657, with running loss: 3.236140727996826\n",
      "Iteration: 306/2657, with running loss: 2.596038818359375\n",
      "Iteration: 307/2657, with running loss: 3.468252658843994\n",
      "Iteration: 308/2657, with running loss: 3.0135741233825684\n",
      "Iteration: 309/2657, with running loss: 2.691589593887329\n",
      "Iteration: 310/2657, with running loss: 2.1430439949035645\n",
      "Iteration: 311/2657, with running loss: 2.764909505844116\n",
      "Iteration: 312/2657, with running loss: 3.1350412368774414\n",
      "Iteration: 313/2657, with running loss: 3.507962942123413\n",
      "Iteration: 314/2657, with running loss: 3.517350673675537\n",
      "Iteration: 315/2657, with running loss: 3.2992796897888184\n",
      "Iteration: 316/2657, with running loss: 2.405155658721924\n",
      "Iteration: 317/2657, with running loss: 2.6145102977752686\n",
      "Iteration: 318/2657, with running loss: 2.1743595600128174\n",
      "Iteration: 319/2657, with running loss: 3.487513303756714\n",
      "Iteration: 320/2657, with running loss: 2.743764638900757\n",
      "Iteration: 321/2657, with running loss: 2.481684684753418\n",
      "Iteration: 322/2657, with running loss: 2.993560314178467\n",
      "Iteration: 323/2657, with running loss: 3.048261880874634\n",
      "Iteration: 324/2657, with running loss: 2.7146496772766113\n",
      "Iteration: 325/2657, with running loss: 2.8154046535491943\n",
      "Iteration: 326/2657, with running loss: 3.298983335494995\n",
      "Iteration: 327/2657, with running loss: 2.4683053493499756\n",
      "Iteration: 328/2657, with running loss: 3.1310319900512695\n",
      "Iteration: 329/2657, with running loss: 2.33073091506958\n",
      "Iteration: 330/2657, with running loss: 2.8022165298461914\n",
      "Iteration: 331/2657, with running loss: 3.775787115097046\n",
      "Iteration: 332/2657, with running loss: 3.418377637863159\n",
      "Iteration: 333/2657, with running loss: 2.469468116760254\n",
      "Iteration: 334/2657, with running loss: 3.8894290924072266\n",
      "Iteration: 335/2657, with running loss: 3.357804298400879\n",
      "Iteration: 336/2657, with running loss: 3.8803462982177734\n",
      "Iteration: 337/2657, with running loss: 3.3635077476501465\n",
      "Iteration: 338/2657, with running loss: 2.9616215229034424\n",
      "Iteration: 339/2657, with running loss: 3.069007635116577\n",
      "Iteration: 340/2657, with running loss: 2.990410566329956\n",
      "Iteration: 341/2657, with running loss: 3.28839111328125\n",
      "Iteration: 342/2657, with running loss: 2.8060519695281982\n",
      "Iteration: 343/2657, with running loss: 3.5747733116149902\n",
      "Iteration: 344/2657, with running loss: 3.0218770503997803\n",
      "Iteration: 345/2657, with running loss: 3.285576581954956\n",
      "Iteration: 346/2657, with running loss: 3.153292655944824\n",
      "Iteration: 347/2657, with running loss: 2.9776861667633057\n",
      "Iteration: 348/2657, with running loss: 3.2673704624176025\n",
      "Iteration: 349/2657, with running loss: 2.4319846630096436\n",
      "Iteration: 350/2657, with running loss: 2.1609575748443604\n",
      "Iteration: 351/2657, with running loss: 2.7005224227905273\n",
      "Iteration: 352/2657, with running loss: 2.5760927200317383\n",
      "Iteration: 353/2657, with running loss: 3.547617197036743\n",
      "Iteration: 354/2657, with running loss: 2.7140085697174072\n",
      "Iteration: 355/2657, with running loss: 3.289412260055542\n",
      "Iteration: 356/2657, with running loss: 2.351119041442871\n",
      "Iteration: 357/2657, with running loss: 2.702605724334717\n",
      "Iteration: 358/2657, with running loss: 3.0998847484588623\n",
      "Iteration: 359/2657, with running loss: 3.9363529682159424\n",
      "Iteration: 360/2657, with running loss: 3.5904972553253174\n",
      "Iteration: 361/2657, with running loss: 3.2822794914245605\n",
      "Iteration: 362/2657, with running loss: 3.2330482006073\n",
      "Iteration: 363/2657, with running loss: 3.716919422149658\n",
      "Iteration: 364/2657, with running loss: 3.2580885887145996\n",
      "Iteration: 365/2657, with running loss: 3.4342658519744873\n",
      "Iteration: 366/2657, with running loss: 2.5189645290374756\n",
      "Iteration: 367/2657, with running loss: 3.4818789958953857\n",
      "Iteration: 368/2657, with running loss: 2.86517071723938\n",
      "Iteration: 369/2657, with running loss: 2.5885705947875977\n",
      "Iteration: 370/2657, with running loss: 2.884397029876709\n",
      "Iteration: 371/2657, with running loss: 3.0821218490600586\n",
      "Iteration: 372/2657, with running loss: 3.6751625537872314\n",
      "Iteration: 373/2657, with running loss: 3.4871649742126465\n",
      "Iteration: 374/2657, with running loss: 3.4479944705963135\n",
      "Iteration: 375/2657, with running loss: 3.525899887084961\n",
      "Iteration: 376/2657, with running loss: 2.7629921436309814\n",
      "Iteration: 377/2657, with running loss: 3.3525311946868896\n",
      "Iteration: 378/2657, with running loss: 3.1667721271514893\n",
      "Iteration: 379/2657, with running loss: 3.846095561981201\n",
      "Iteration: 380/2657, with running loss: 3.266164541244507\n",
      "Iteration: 381/2657, with running loss: 3.518554925918579\n",
      "Iteration: 382/2657, with running loss: 3.924233913421631\n",
      "Iteration: 383/2657, with running loss: 2.6705384254455566\n",
      "Iteration: 384/2657, with running loss: 3.659467935562134\n",
      "Iteration: 385/2657, with running loss: 3.5126614570617676\n",
      "Iteration: 386/2657, with running loss: 2.7521893978118896\n",
      "Iteration: 387/2657, with running loss: 4.025360584259033\n",
      "Iteration: 388/2657, with running loss: 3.239762783050537\n",
      "Iteration: 389/2657, with running loss: 3.197049379348755\n",
      "Iteration: 390/2657, with running loss: 2.3290469646453857\n",
      "Iteration: 391/2657, with running loss: 3.6066205501556396\n",
      "Iteration: 392/2657, with running loss: 3.9180736541748047\n",
      "Iteration: 393/2657, with running loss: 2.620269298553467\n",
      "Iteration: 394/2657, with running loss: 3.0814647674560547\n",
      "Iteration: 395/2657, with running loss: 3.6451971530914307\n",
      "Iteration: 396/2657, with running loss: 2.687302827835083\n",
      "Iteration: 397/2657, with running loss: 3.5968971252441406\n",
      "Iteration: 398/2657, with running loss: 2.950089693069458\n",
      "Iteration: 399/2657, with running loss: 1.9968093633651733\n",
      "Iteration: 400/2657, with running loss: 3.4911210536956787\n",
      "Iteration: 401/2657, with running loss: 3.043200731277466\n",
      "Iteration: 402/2657, with running loss: 2.914766788482666\n",
      "Iteration: 403/2657, with running loss: 2.7142536640167236\n",
      "Iteration: 404/2657, with running loss: 2.409454107284546\n",
      "Iteration: 405/2657, with running loss: 3.095196485519409\n",
      "Iteration: 406/2657, with running loss: 2.3521454334259033\n",
      "Iteration: 407/2657, with running loss: 3.051513910293579\n",
      "Iteration: 408/2657, with running loss: 3.725857734680176\n",
      "Iteration: 409/2657, with running loss: 3.403031349182129\n",
      "Iteration: 410/2657, with running loss: 3.207831621170044\n",
      "Iteration: 411/2657, with running loss: 2.0757625102996826\n",
      "Iteration: 412/2657, with running loss: 1.9261146783828735\n",
      "Iteration: 413/2657, with running loss: 3.0528547763824463\n",
      "Iteration: 414/2657, with running loss: 2.678586959838867\n",
      "Iteration: 415/2657, with running loss: 2.6433379650115967\n",
      "Iteration: 416/2657, with running loss: 2.8674020767211914\n",
      "Iteration: 417/2657, with running loss: 3.0658645629882812\n",
      "Iteration: 418/2657, with running loss: 2.874284029006958\n",
      "Iteration: 419/2657, with running loss: 2.9311671257019043\n",
      "Iteration: 420/2657, with running loss: 2.9562649726867676\n",
      "Iteration: 421/2657, with running loss: 2.8395164012908936\n",
      "Iteration: 422/2657, with running loss: 2.8500683307647705\n",
      "Iteration: 423/2657, with running loss: 2.9374053478240967\n",
      "Iteration: 424/2657, with running loss: 2.822273015975952\n",
      "Iteration: 425/2657, with running loss: 2.761327028274536\n",
      "Iteration: 426/2657, with running loss: 3.9235939979553223\n",
      "Iteration: 427/2657, with running loss: 3.821296215057373\n",
      "Iteration: 428/2657, with running loss: 2.5208473205566406\n",
      "Iteration: 429/2657, with running loss: 2.6785569190979004\n",
      "Iteration: 430/2657, with running loss: 3.32474684715271\n",
      "Iteration: 431/2657, with running loss: 2.167228937149048\n",
      "Iteration: 432/2657, with running loss: 1.7289979457855225\n",
      "Iteration: 433/2657, with running loss: 2.798686981201172\n",
      "Iteration: 434/2657, with running loss: 3.490504741668701\n",
      "Iteration: 435/2657, with running loss: 2.704958438873291\n",
      "Iteration: 436/2657, with running loss: 2.5733323097229004\n",
      "Iteration: 437/2657, with running loss: 2.813783645629883\n",
      "Iteration: 438/2657, with running loss: 3.2824716567993164\n",
      "Iteration: 439/2657, with running loss: 2.7885313034057617\n",
      "Iteration: 440/2657, with running loss: 2.166053295135498\n",
      "Iteration: 441/2657, with running loss: 3.306650161743164\n",
      "Iteration: 442/2657, with running loss: 1.9402823448181152\n",
      "Iteration: 443/2657, with running loss: 3.417506217956543\n",
      "Iteration: 444/2657, with running loss: 2.7269866466522217\n",
      "Iteration: 445/2657, with running loss: 3.549203872680664\n",
      "Iteration: 446/2657, with running loss: 3.5877811908721924\n",
      "Iteration: 447/2657, with running loss: 2.894707679748535\n",
      "Iteration: 448/2657, with running loss: 3.2036941051483154\n",
      "Iteration: 449/2657, with running loss: 3.5327398777008057\n",
      "Iteration: 450/2657, with running loss: 2.4963619709014893\n",
      "Iteration: 451/2657, with running loss: 2.928821563720703\n",
      "Iteration: 452/2657, with running loss: 2.263495683670044\n",
      "Iteration: 453/2657, with running loss: 3.7393577098846436\n",
      "Iteration: 454/2657, with running loss: 3.1616263389587402\n",
      "Iteration: 455/2657, with running loss: 2.5345163345336914\n",
      "Iteration: 456/2657, with running loss: 3.139137029647827\n",
      "Iteration: 457/2657, with running loss: 2.5204474925994873\n",
      "Iteration: 458/2657, with running loss: 2.5310218334198\n",
      "Iteration: 459/2657, with running loss: 3.206340789794922\n",
      "Iteration: 460/2657, with running loss: 3.233065128326416\n",
      "Iteration: 461/2657, with running loss: 2.9061977863311768\n",
      "Iteration: 462/2657, with running loss: 3.168009042739868\n",
      "Iteration: 463/2657, with running loss: 3.099416494369507\n",
      "Iteration: 464/2657, with running loss: 3.6588141918182373\n",
      "Iteration: 465/2657, with running loss: 2.7411839962005615\n",
      "Iteration: 466/2657, with running loss: 2.9534478187561035\n",
      "Iteration: 467/2657, with running loss: 3.1551928520202637\n",
      "Iteration: 468/2657, with running loss: 3.2442626953125\n",
      "Iteration: 469/2657, with running loss: 2.4600894451141357\n",
      "Iteration: 470/2657, with running loss: 3.1290814876556396\n",
      "Iteration: 471/2657, with running loss: 2.4717557430267334\n",
      "Iteration: 472/2657, with running loss: 2.1245694160461426\n",
      "Iteration: 473/2657, with running loss: 2.6830458641052246\n",
      "Iteration: 474/2657, with running loss: 3.030518054962158\n",
      "Iteration: 475/2657, with running loss: 3.4688615798950195\n",
      "Iteration: 476/2657, with running loss: 3.562035322189331\n",
      "Iteration: 477/2657, with running loss: 3.4305572509765625\n",
      "Iteration: 478/2657, with running loss: 3.7008886337280273\n",
      "Iteration: 479/2657, with running loss: 2.9080567359924316\n",
      "Iteration: 480/2657, with running loss: 3.041944980621338\n",
      "Iteration: 481/2657, with running loss: 3.435647964477539\n",
      "Iteration: 482/2657, with running loss: 2.912045478820801\n",
      "Iteration: 483/2657, with running loss: 3.3705363273620605\n",
      "Iteration: 484/2657, with running loss: 3.265192985534668\n",
      "Iteration: 485/2657, with running loss: 3.8543124198913574\n",
      "Iteration: 486/2657, with running loss: 2.3042712211608887\n",
      "Iteration: 487/2657, with running loss: 3.3065502643585205\n",
      "Iteration: 488/2657, with running loss: 3.2854535579681396\n",
      "Iteration: 489/2657, with running loss: 4.037588119506836\n",
      "Iteration: 490/2657, with running loss: 3.272968053817749\n",
      "Iteration: 491/2657, with running loss: 2.4996562004089355\n",
      "Iteration: 492/2657, with running loss: 3.3365955352783203\n",
      "Iteration: 493/2657, with running loss: 2.3805971145629883\n",
      "Iteration: 494/2657, with running loss: 3.5868701934814453\n",
      "Iteration: 495/2657, with running loss: 2.8788602352142334\n",
      "Iteration: 496/2657, with running loss: 2.8042283058166504\n",
      "Iteration: 497/2657, with running loss: 3.700836658477783\n",
      "Iteration: 498/2657, with running loss: 2.858564615249634\n",
      "Iteration: 499/2657, with running loss: 3.6133575439453125\n",
      "Iteration: 500/2657, with running loss: 2.6361331939697266\n",
      "Iteration: 501/2657, with running loss: 3.573197364807129\n",
      "Iteration: 502/2657, with running loss: 2.553272008895874\n",
      "Iteration: 503/2657, with running loss: 2.684267282485962\n",
      "Iteration: 504/2657, with running loss: 3.751925230026245\n",
      "Iteration: 505/2657, with running loss: 2.692673921585083\n",
      "Iteration: 506/2657, with running loss: 2.5031018257141113\n",
      "Iteration: 507/2657, with running loss: 3.4887185096740723\n",
      "Iteration: 508/2657, with running loss: 3.336805820465088\n",
      "Iteration: 509/2657, with running loss: 3.385526180267334\n",
      "Iteration: 510/2657, with running loss: 2.9605391025543213\n",
      "Iteration: 511/2657, with running loss: 3.1637356281280518\n",
      "Iteration: 512/2657, with running loss: 2.888493776321411\n",
      "Iteration: 513/2657, with running loss: 2.524538516998291\n",
      "Iteration: 514/2657, with running loss: 2.487285614013672\n",
      "Iteration: 515/2657, with running loss: 3.1684978008270264\n",
      "Iteration: 516/2657, with running loss: 2.8329291343688965\n",
      "Iteration: 517/2657, with running loss: 3.566035270690918\n",
      "Iteration: 518/2657, with running loss: 2.6470000743865967\n",
      "Iteration: 519/2657, with running loss: 2.7035224437713623\n",
      "Iteration: 520/2657, with running loss: 2.9691994190216064\n",
      "Iteration: 521/2657, with running loss: 2.384136199951172\n",
      "Iteration: 522/2657, with running loss: 2.494990825653076\n",
      "Iteration: 523/2657, with running loss: 2.7440261840820312\n",
      "Iteration: 524/2657, with running loss: 3.066338062286377\n",
      "Iteration: 525/2657, with running loss: 3.2372255325317383\n",
      "Iteration: 526/2657, with running loss: 3.2914230823516846\n",
      "Iteration: 527/2657, with running loss: 3.389068603515625\n",
      "Iteration: 528/2657, with running loss: 2.6345369815826416\n",
      "Iteration: 529/2657, with running loss: 2.952868700027466\n",
      "Iteration: 530/2657, with running loss: 3.0336761474609375\n",
      "Iteration: 531/2657, with running loss: 2.6348841190338135\n",
      "Iteration: 532/2657, with running loss: 3.5553009510040283\n",
      "Iteration: 533/2657, with running loss: 3.6708920001983643\n",
      "Iteration: 534/2657, with running loss: 3.5628089904785156\n",
      "Iteration: 535/2657, with running loss: 1.826706051826477\n",
      "Iteration: 536/2657, with running loss: 2.0158536434173584\n",
      "Iteration: 537/2657, with running loss: 2.6812946796417236\n",
      "Iteration: 538/2657, with running loss: 2.816314458847046\n",
      "Iteration: 539/2657, with running loss: 3.6807949542999268\n",
      "Iteration: 540/2657, with running loss: 2.700495719909668\n",
      "Iteration: 541/2657, with running loss: 2.250795841217041\n",
      "Iteration: 542/2657, with running loss: 3.584155797958374\n",
      "Iteration: 543/2657, with running loss: 3.757173538208008\n",
      "Iteration: 544/2657, with running loss: 3.180591344833374\n",
      "Iteration: 545/2657, with running loss: 2.768203020095825\n",
      "Iteration: 546/2657, with running loss: 3.707279682159424\n",
      "Iteration: 547/2657, with running loss: 3.0244104862213135\n",
      "Iteration: 548/2657, with running loss: 3.5405075550079346\n",
      "Iteration: 549/2657, with running loss: 3.1742758750915527\n",
      "Iteration: 550/2657, with running loss: 3.9758923053741455\n",
      "Iteration: 551/2657, with running loss: 3.322622060775757\n",
      "Iteration: 552/2657, with running loss: 3.535954236984253\n",
      "Iteration: 553/2657, with running loss: 2.8509867191314697\n",
      "Iteration: 554/2657, with running loss: 2.797008752822876\n",
      "Iteration: 555/2657, with running loss: 3.0068728923797607\n",
      "Iteration: 556/2657, with running loss: 2.552558660507202\n",
      "Iteration: 557/2657, with running loss: 1.9145760536193848\n",
      "Iteration: 558/2657, with running loss: 3.199692487716675\n",
      "Iteration: 559/2657, with running loss: 4.125390529632568\n",
      "Iteration: 560/2657, with running loss: 2.655059814453125\n",
      "Iteration: 561/2657, with running loss: 3.565159559249878\n",
      "Iteration: 562/2657, with running loss: 3.300893545150757\n",
      "Iteration: 563/2657, with running loss: 4.025617599487305\n",
      "Iteration: 564/2657, with running loss: 2.6353447437286377\n",
      "Iteration: 565/2657, with running loss: 3.049145221710205\n",
      "Iteration: 566/2657, with running loss: 2.8798575401306152\n",
      "Iteration: 567/2657, with running loss: 3.141664743423462\n",
      "Iteration: 568/2657, with running loss: 2.693094491958618\n",
      "Iteration: 569/2657, with running loss: 3.4665629863739014\n",
      "Iteration: 570/2657, with running loss: 4.030186653137207\n",
      "Iteration: 571/2657, with running loss: 3.569859504699707\n",
      "Iteration: 572/2657, with running loss: 2.834846258163452\n",
      "Iteration: 573/2657, with running loss: 2.2586076259613037\n",
      "Iteration: 574/2657, with running loss: 2.7902793884277344\n",
      "Iteration: 575/2657, with running loss: 2.7837002277374268\n",
      "Iteration: 576/2657, with running loss: 3.1920108795166016\n",
      "Iteration: 577/2657, with running loss: 3.2301034927368164\n",
      "Iteration: 578/2657, with running loss: 3.2729578018188477\n",
      "Iteration: 579/2657, with running loss: 2.7615139484405518\n",
      "Iteration: 580/2657, with running loss: 3.432095766067505\n",
      "Iteration: 581/2657, with running loss: 3.0860183238983154\n",
      "Iteration: 582/2657, with running loss: 3.40681791305542\n",
      "Iteration: 583/2657, with running loss: 2.629150867462158\n",
      "Iteration: 584/2657, with running loss: 3.630143880844116\n",
      "Iteration: 585/2657, with running loss: 3.275190591812134\n",
      "Iteration: 586/2657, with running loss: 2.6126790046691895\n",
      "Iteration: 587/2657, with running loss: 2.1980276107788086\n",
      "Iteration: 588/2657, with running loss: 3.9408202171325684\n",
      "Iteration: 589/2657, with running loss: 4.202486515045166\n",
      "Iteration: 590/2657, with running loss: 1.9039242267608643\n",
      "Iteration: 591/2657, with running loss: 3.6704540252685547\n",
      "Iteration: 592/2657, with running loss: 3.7010350227355957\n",
      "Iteration: 593/2657, with running loss: 3.607560157775879\n",
      "Iteration: 594/2657, with running loss: 3.0984411239624023\n",
      "Iteration: 595/2657, with running loss: 3.0171124935150146\n",
      "Iteration: 596/2657, with running loss: 3.991564989089966\n",
      "Iteration: 597/2657, with running loss: 2.856860399246216\n",
      "Iteration: 598/2657, with running loss: 3.419694185256958\n",
      "Iteration: 599/2657, with running loss: 3.3871254920959473\n",
      "Iteration: 600/2657, with running loss: 3.8740122318267822\n",
      "Iteration: 601/2657, with running loss: 3.1668648719787598\n",
      "Iteration: 602/2657, with running loss: 3.5196139812469482\n",
      "Iteration: 603/2657, with running loss: 3.5301806926727295\n",
      "Iteration: 604/2657, with running loss: 2.961991310119629\n",
      "Iteration: 605/2657, with running loss: 2.0504419803619385\n",
      "Iteration: 606/2657, with running loss: 2.825144052505493\n",
      "Iteration: 607/2657, with running loss: 3.4224560260772705\n",
      "Iteration: 608/2657, with running loss: 3.2435944080352783\n",
      "Iteration: 609/2657, with running loss: 3.4713547229766846\n",
      "Iteration: 610/2657, with running loss: 2.9749441146850586\n",
      "Iteration: 611/2657, with running loss: 3.3133528232574463\n",
      "Iteration: 612/2657, with running loss: 3.286052942276001\n",
      "Iteration: 613/2657, with running loss: 3.945661783218384\n",
      "Iteration: 614/2657, with running loss: 3.4677791595458984\n",
      "Iteration: 615/2657, with running loss: 3.800187587738037\n",
      "Iteration: 616/2657, with running loss: 3.12524676322937\n",
      "Iteration: 617/2657, with running loss: 3.1009392738342285\n",
      "Iteration: 618/2657, with running loss: 2.984995126724243\n",
      "Iteration: 619/2657, with running loss: 3.3544957637786865\n",
      "Iteration: 620/2657, with running loss: 3.3809263706207275\n",
      "Iteration: 621/2657, with running loss: 3.126324415206909\n",
      "Iteration: 622/2657, with running loss: 3.124849557876587\n",
      "Iteration: 623/2657, with running loss: 2.7054390907287598\n",
      "Iteration: 624/2657, with running loss: 3.933603048324585\n",
      "Iteration: 625/2657, with running loss: 3.4981253147125244\n",
      "Iteration: 626/2657, with running loss: 2.880687713623047\n",
      "Iteration: 627/2657, with running loss: 2.448883295059204\n",
      "Iteration: 628/2657, with running loss: 3.352494955062866\n",
      "Iteration: 629/2657, with running loss: 2.81949782371521\n",
      "Iteration: 630/2657, with running loss: 2.901538848876953\n",
      "Iteration: 631/2657, with running loss: 3.547802686691284\n",
      "Iteration: 632/2657, with running loss: 3.5886471271514893\n",
      "Iteration: 633/2657, with running loss: 2.843158483505249\n",
      "Iteration: 634/2657, with running loss: 3.2063064575195312\n",
      "Iteration: 635/2657, with running loss: 2.6626527309417725\n",
      "Iteration: 636/2657, with running loss: 3.3558623790740967\n",
      "Iteration: 637/2657, with running loss: 2.9258522987365723\n",
      "Iteration: 638/2657, with running loss: 3.3025286197662354\n",
      "Iteration: 639/2657, with running loss: 3.144397497177124\n",
      "Iteration: 640/2657, with running loss: 3.5164310932159424\n",
      "Iteration: 641/2657, with running loss: 3.0201597213745117\n",
      "Iteration: 642/2657, with running loss: 2.866304397583008\n",
      "Iteration: 643/2657, with running loss: 2.3860292434692383\n",
      "Iteration: 644/2657, with running loss: 2.8992135524749756\n",
      "Iteration: 645/2657, with running loss: 3.087268352508545\n",
      "Iteration: 646/2657, with running loss: 3.0526325702667236\n",
      "Iteration: 647/2657, with running loss: 3.481219530105591\n",
      "Iteration: 648/2657, with running loss: 3.4278464317321777\n",
      "Iteration: 649/2657, with running loss: 3.269721269607544\n",
      "Iteration: 650/2657, with running loss: 3.1654443740844727\n",
      "Iteration: 651/2657, with running loss: 2.7149245738983154\n",
      "Iteration: 652/2657, with running loss: 2.955545663833618\n",
      "Iteration: 653/2657, with running loss: 3.2700905799865723\n",
      "Iteration: 654/2657, with running loss: 3.5393993854522705\n",
      "Iteration: 655/2657, with running loss: 2.8658132553100586\n",
      "Iteration: 656/2657, with running loss: 3.158287525177002\n",
      "Iteration: 657/2657, with running loss: 2.9330356121063232\n",
      "Iteration: 658/2657, with running loss: 3.435049057006836\n",
      "Iteration: 659/2657, with running loss: 3.771770715713501\n",
      "Iteration: 660/2657, with running loss: 3.2052102088928223\n",
      "Iteration: 661/2657, with running loss: 2.82755970954895\n",
      "Iteration: 662/2657, with running loss: 3.44376540184021\n",
      "Iteration: 663/2657, with running loss: 3.683534622192383\n",
      "Iteration: 664/2657, with running loss: 2.6334493160247803\n",
      "Iteration: 665/2657, with running loss: 2.0354549884796143\n",
      "Iteration: 666/2657, with running loss: 2.7952005863189697\n",
      "Iteration: 667/2657, with running loss: 2.8096842765808105\n",
      "Iteration: 668/2657, with running loss: 2.892533779144287\n",
      "Iteration: 669/2657, with running loss: 2.2804863452911377\n",
      "Iteration: 670/2657, with running loss: 2.9911530017852783\n",
      "Iteration: 671/2657, with running loss: 2.9626102447509766\n",
      "Iteration: 672/2657, with running loss: 3.123091697692871\n",
      "Iteration: 673/2657, with running loss: 3.309553623199463\n",
      "Iteration: 674/2657, with running loss: 2.7973995208740234\n",
      "Iteration: 675/2657, with running loss: 2.410931348800659\n",
      "Iteration: 676/2657, with running loss: 3.2370715141296387\n",
      "Iteration: 677/2657, with running loss: 3.330721139907837\n",
      "Iteration: 678/2657, with running loss: 2.4918265342712402\n",
      "Iteration: 679/2657, with running loss: 2.7972047328948975\n",
      "Iteration: 680/2657, with running loss: 3.144670009613037\n",
      "Iteration: 681/2657, with running loss: 3.31589674949646\n",
      "Iteration: 682/2657, with running loss: 3.791372776031494\n",
      "Iteration: 683/2657, with running loss: 2.0225558280944824\n",
      "Iteration: 684/2657, with running loss: 3.1667706966400146\n",
      "Iteration: 685/2657, with running loss: 3.368807792663574\n",
      "Iteration: 686/2657, with running loss: 2.7962515354156494\n",
      "Iteration: 687/2657, with running loss: 2.0230913162231445\n",
      "Iteration: 688/2657, with running loss: 2.595306873321533\n",
      "Iteration: 689/2657, with running loss: 3.6818952560424805\n",
      "Iteration: 690/2657, with running loss: 2.951064348220825\n",
      "Iteration: 691/2657, with running loss: 2.9712066650390625\n",
      "Iteration: 692/2657, with running loss: 3.620276927947998\n",
      "Iteration: 693/2657, with running loss: 3.8213040828704834\n",
      "Iteration: 694/2657, with running loss: 3.1412410736083984\n",
      "Iteration: 695/2657, with running loss: 3.460688829421997\n",
      "Iteration: 696/2657, with running loss: 3.021940231323242\n",
      "Iteration: 697/2657, with running loss: 2.7471554279327393\n",
      "Iteration: 698/2657, with running loss: 3.0901124477386475\n",
      "Iteration: 699/2657, with running loss: 2.4897494316101074\n",
      "Iteration: 700/2657, with running loss: 3.5820908546447754\n",
      "Iteration: 701/2657, with running loss: 2.9638092517852783\n",
      "Iteration: 702/2657, with running loss: 3.0467913150787354\n",
      "Iteration: 703/2657, with running loss: 3.120682716369629\n",
      "Iteration: 704/2657, with running loss: 2.314689874649048\n",
      "Iteration: 705/2657, with running loss: 3.5061542987823486\n",
      "Iteration: 706/2657, with running loss: 2.9417033195495605\n",
      "Iteration: 707/2657, with running loss: 2.0470311641693115\n",
      "Iteration: 708/2657, with running loss: 2.9348974227905273\n",
      "Iteration: 709/2657, with running loss: 3.2542061805725098\n",
      "Iteration: 710/2657, with running loss: 2.791318416595459\n",
      "Iteration: 711/2657, with running loss: 2.474485158920288\n",
      "Iteration: 712/2657, with running loss: 3.560783863067627\n",
      "Iteration: 713/2657, with running loss: 3.2600231170654297\n",
      "Iteration: 714/2657, with running loss: 3.0122597217559814\n",
      "Iteration: 715/2657, with running loss: 2.584651231765747\n",
      "Iteration: 716/2657, with running loss: 2.7233164310455322\n",
      "Iteration: 717/2657, with running loss: 2.971370220184326\n",
      "Iteration: 718/2657, with running loss: 2.7772181034088135\n",
      "Iteration: 719/2657, with running loss: 2.8274295330047607\n",
      "Iteration: 720/2657, with running loss: 3.011150598526001\n",
      "Iteration: 721/2657, with running loss: 3.000169515609741\n",
      "Iteration: 722/2657, with running loss: 2.638950824737549\n",
      "Iteration: 723/2657, with running loss: 3.29205060005188\n",
      "Iteration: 724/2657, with running loss: 2.519240140914917\n",
      "Iteration: 725/2657, with running loss: 3.5997955799102783\n",
      "Iteration: 726/2657, with running loss: 3.1443593502044678\n",
      "Iteration: 727/2657, with running loss: 3.1619582176208496\n",
      "Iteration: 728/2657, with running loss: 1.9061671495437622\n",
      "Iteration: 729/2657, with running loss: 2.4984707832336426\n",
      "Iteration: 730/2657, with running loss: 2.546771287918091\n",
      "Iteration: 731/2657, with running loss: 2.5980708599090576\n",
      "Iteration: 732/2657, with running loss: 3.0641987323760986\n",
      "Iteration: 733/2657, with running loss: 2.717576026916504\n",
      "Iteration: 734/2657, with running loss: 2.3016932010650635\n",
      "Iteration: 735/2657, with running loss: 2.0691378116607666\n",
      "Iteration: 736/2657, with running loss: 3.487873077392578\n",
      "Iteration: 737/2657, with running loss: 2.8922150135040283\n",
      "Iteration: 738/2657, with running loss: 3.6716418266296387\n",
      "Iteration: 739/2657, with running loss: 1.8130460977554321\n",
      "Iteration: 740/2657, with running loss: 3.6889636516571045\n",
      "Iteration: 741/2657, with running loss: 3.3706507682800293\n",
      "Iteration: 742/2657, with running loss: 3.7489540576934814\n",
      "Iteration: 743/2657, with running loss: 3.005387306213379\n",
      "Iteration: 744/2657, with running loss: 3.252652883529663\n",
      "Iteration: 745/2657, with running loss: 3.325024366378784\n",
      "Iteration: 746/2657, with running loss: 2.8336262702941895\n",
      "Iteration: 747/2657, with running loss: 3.5355658531188965\n",
      "Iteration: 748/2657, with running loss: 3.0208022594451904\n",
      "Iteration: 749/2657, with running loss: 3.4395999908447266\n",
      "Iteration: 750/2657, with running loss: 3.147202253341675\n",
      "Iteration: 751/2657, with running loss: 3.0908796787261963\n",
      "Iteration: 752/2657, with running loss: 3.0404350757598877\n",
      "Iteration: 753/2657, with running loss: 3.670341968536377\n",
      "Iteration: 754/2657, with running loss: 2.164202928543091\n",
      "Iteration: 755/2657, with running loss: 2.650712728500366\n",
      "Iteration: 756/2657, with running loss: 2.131932497024536\n",
      "Iteration: 757/2657, with running loss: 3.0129969120025635\n",
      "Iteration: 758/2657, with running loss: 3.069149971008301\n",
      "Iteration: 759/2657, with running loss: 2.6544923782348633\n",
      "Iteration: 760/2657, with running loss: 3.1197586059570312\n",
      "Iteration: 761/2657, with running loss: 3.1711528301239014\n",
      "Iteration: 762/2657, with running loss: 3.6518561840057373\n",
      "Iteration: 763/2657, with running loss: 2.727647066116333\n",
      "Iteration: 764/2657, with running loss: 2.960068702697754\n",
      "Iteration: 765/2657, with running loss: 3.1651012897491455\n",
      "Iteration: 766/2657, with running loss: 2.1852314472198486\n",
      "Iteration: 767/2657, with running loss: 2.9371395111083984\n",
      "Iteration: 768/2657, with running loss: 3.6244008541107178\n",
      "Iteration: 769/2657, with running loss: 3.322345018386841\n",
      "Iteration: 770/2657, with running loss: 2.4193761348724365\n",
      "Iteration: 771/2657, with running loss: 2.695331335067749\n",
      "Iteration: 772/2657, with running loss: 2.4903452396392822\n",
      "Iteration: 773/2657, with running loss: 2.6256654262542725\n",
      "Iteration: 774/2657, with running loss: 3.140521764755249\n",
      "Iteration: 775/2657, with running loss: 2.857761859893799\n",
      "Iteration: 776/2657, with running loss: 2.457141399383545\n",
      "Iteration: 777/2657, with running loss: 3.392852544784546\n",
      "Iteration: 778/2657, with running loss: 2.7253434658050537\n",
      "Iteration: 779/2657, with running loss: 3.581592321395874\n",
      "Iteration: 780/2657, with running loss: 3.4045839309692383\n",
      "Iteration: 781/2657, with running loss: 2.916560411453247\n",
      "Iteration: 782/2657, with running loss: 3.0030858516693115\n",
      "Iteration: 783/2657, with running loss: 3.4118213653564453\n",
      "Iteration: 784/2657, with running loss: 4.257256031036377\n",
      "Iteration: 785/2657, with running loss: 2.519706964492798\n",
      "Iteration: 786/2657, with running loss: 4.1263957023620605\n",
      "Iteration: 787/2657, with running loss: 3.3120408058166504\n",
      "Iteration: 788/2657, with running loss: 2.5046613216400146\n",
      "Iteration: 789/2657, with running loss: 2.3129754066467285\n",
      "Iteration: 790/2657, with running loss: 3.288945436477661\n",
      "Iteration: 791/2657, with running loss: 3.202286720275879\n",
      "Iteration: 792/2657, with running loss: 3.5085272789001465\n",
      "Iteration: 793/2657, with running loss: 2.5416135787963867\n",
      "Iteration: 794/2657, with running loss: 3.0819826126098633\n",
      "Iteration: 795/2657, with running loss: 2.4560420513153076\n",
      "Iteration: 796/2657, with running loss: 3.243438720703125\n",
      "Iteration: 797/2657, with running loss: 2.689296245574951\n",
      "Iteration: 798/2657, with running loss: 3.3948886394500732\n",
      "Iteration: 799/2657, with running loss: 2.8878908157348633\n",
      "Iteration: 800/2657, with running loss: 3.0330629348754883\n",
      "Iteration: 801/2657, with running loss: 3.3956944942474365\n",
      "Iteration: 802/2657, with running loss: 3.174751043319702\n",
      "Iteration: 803/2657, with running loss: 2.968684434890747\n",
      "Iteration: 804/2657, with running loss: 2.809297800064087\n",
      "Iteration: 805/2657, with running loss: 3.5576975345611572\n",
      "Iteration: 806/2657, with running loss: 2.561673164367676\n",
      "Iteration: 807/2657, with running loss: 2.756596565246582\n",
      "Iteration: 808/2657, with running loss: 3.5384328365325928\n",
      "Iteration: 809/2657, with running loss: 3.153294324874878\n",
      "Iteration: 810/2657, with running loss: 2.55995512008667\n",
      "Iteration: 811/2657, with running loss: 3.165914535522461\n",
      "Iteration: 812/2657, with running loss: 2.7007927894592285\n",
      "Iteration: 813/2657, with running loss: 3.5075557231903076\n",
      "Iteration: 814/2657, with running loss: 2.9579696655273438\n",
      "Iteration: 815/2657, with running loss: 3.363518476486206\n",
      "Iteration: 816/2657, with running loss: 3.4853625297546387\n",
      "Iteration: 817/2657, with running loss: 3.512404680252075\n",
      "Iteration: 818/2657, with running loss: 2.614072799682617\n",
      "Iteration: 819/2657, with running loss: 2.8489973545074463\n",
      "Iteration: 820/2657, with running loss: 3.576307535171509\n",
      "Iteration: 821/2657, with running loss: 3.101151704788208\n",
      "Iteration: 822/2657, with running loss: 3.077362298965454\n",
      "Iteration: 823/2657, with running loss: 3.4163787364959717\n",
      "Iteration: 824/2657, with running loss: 2.286658763885498\n",
      "Iteration: 825/2657, with running loss: 3.4386942386627197\n",
      "Iteration: 826/2657, with running loss: 3.8290724754333496\n",
      "Iteration: 827/2657, with running loss: 2.9326438903808594\n",
      "Iteration: 828/2657, with running loss: 2.866131544113159\n",
      "Iteration: 829/2657, with running loss: 4.19248104095459\n",
      "Iteration: 830/2657, with running loss: 2.635197877883911\n",
      "Iteration: 831/2657, with running loss: 3.4458658695220947\n",
      "Iteration: 832/2657, with running loss: 2.9849512577056885\n",
      "Iteration: 833/2657, with running loss: 3.427290678024292\n",
      "Iteration: 834/2657, with running loss: 2.3683502674102783\n",
      "Iteration: 835/2657, with running loss: 3.1779792308807373\n",
      "Iteration: 836/2657, with running loss: 3.8867759704589844\n",
      "Iteration: 837/2657, with running loss: 2.568986177444458\n",
      "Iteration: 838/2657, with running loss: 3.5080275535583496\n",
      "Iteration: 839/2657, with running loss: 3.309636116027832\n",
      "Iteration: 840/2657, with running loss: 2.56619930267334\n",
      "Iteration: 841/2657, with running loss: 3.6319990158081055\n",
      "Iteration: 842/2657, with running loss: 3.3570878505706787\n",
      "Iteration: 843/2657, with running loss: 3.404165744781494\n",
      "Iteration: 844/2657, with running loss: 3.0213518142700195\n",
      "Iteration: 845/2657, with running loss: 3.0506999492645264\n",
      "Iteration: 846/2657, with running loss: 2.900019884109497\n",
      "Iteration: 847/2657, with running loss: 2.281529664993286\n",
      "Iteration: 848/2657, with running loss: 3.484476089477539\n",
      "Iteration: 849/2657, with running loss: 2.483109474182129\n",
      "Iteration: 850/2657, with running loss: 3.5270497798919678\n",
      "Iteration: 851/2657, with running loss: 2.393120765686035\n",
      "Iteration: 852/2657, with running loss: 2.9203686714172363\n",
      "Iteration: 853/2657, with running loss: 2.9814937114715576\n",
      "Iteration: 854/2657, with running loss: 3.177682876586914\n",
      "Iteration: 855/2657, with running loss: 2.3792951107025146\n",
      "Iteration: 856/2657, with running loss: 2.758005142211914\n",
      "Iteration: 857/2657, with running loss: 3.80979323387146\n",
      "Iteration: 858/2657, with running loss: 3.882697343826294\n",
      "Iteration: 859/2657, with running loss: 3.106806755065918\n",
      "Iteration: 860/2657, with running loss: 2.6537046432495117\n",
      "Iteration: 861/2657, with running loss: 2.428022623062134\n",
      "Iteration: 862/2657, with running loss: 3.2038733959198\n",
      "Iteration: 863/2657, with running loss: 3.018059253692627\n",
      "Iteration: 864/2657, with running loss: 2.9349095821380615\n",
      "Iteration: 865/2657, with running loss: 2.271266460418701\n",
      "Iteration: 866/2657, with running loss: 3.3094990253448486\n",
      "Iteration: 867/2657, with running loss: 2.4953856468200684\n",
      "Iteration: 868/2657, with running loss: 2.790376663208008\n",
      "Iteration: 869/2657, with running loss: 3.045867681503296\n",
      "Iteration: 870/2657, with running loss: 3.1519243717193604\n",
      "Iteration: 871/2657, with running loss: 3.0669286251068115\n",
      "Iteration: 872/2657, with running loss: 3.425236940383911\n",
      "Iteration: 873/2657, with running loss: 3.4522595405578613\n",
      "Iteration: 874/2657, with running loss: 2.5510215759277344\n",
      "Iteration: 875/2657, with running loss: 2.9358627796173096\n",
      "Iteration: 876/2657, with running loss: 3.1657187938690186\n",
      "Iteration: 877/2657, with running loss: 3.3341193199157715\n",
      "Iteration: 878/2657, with running loss: 2.663710355758667\n",
      "Iteration: 879/2657, with running loss: 2.9041945934295654\n",
      "Iteration: 880/2657, with running loss: 3.3000166416168213\n",
      "Iteration: 881/2657, with running loss: 3.965810537338257\n",
      "Iteration: 882/2657, with running loss: 3.7966482639312744\n",
      "Iteration: 883/2657, with running loss: 3.060457468032837\n",
      "Iteration: 884/2657, with running loss: 3.515848159790039\n",
      "Iteration: 885/2657, with running loss: 2.7387804985046387\n",
      "Iteration: 886/2657, with running loss: 3.575136184692383\n",
      "Iteration: 887/2657, with running loss: 3.158261299133301\n",
      "Iteration: 888/2657, with running loss: 2.2791249752044678\n",
      "Iteration: 889/2657, with running loss: 3.3996593952178955\n",
      "Iteration: 890/2657, with running loss: 3.2518882751464844\n",
      "Iteration: 891/2657, with running loss: 2.735607862472534\n",
      "Iteration: 892/2657, with running loss: 2.0988142490386963\n",
      "Iteration: 893/2657, with running loss: 3.1091325283050537\n",
      "Iteration: 894/2657, with running loss: 2.964334726333618\n",
      "Iteration: 895/2657, with running loss: 2.6004045009613037\n",
      "Iteration: 896/2657, with running loss: 3.1659200191497803\n",
      "Iteration: 897/2657, with running loss: 2.373767137527466\n",
      "Iteration: 898/2657, with running loss: 2.5932130813598633\n",
      "Iteration: 899/2657, with running loss: 3.235635995864868\n",
      "Iteration: 900/2657, with running loss: 3.1871094703674316\n",
      "Iteration: 901/2657, with running loss: 2.9419467449188232\n",
      "Iteration: 902/2657, with running loss: 2.5289480686187744\n",
      "Iteration: 903/2657, with running loss: 2.6743369102478027\n",
      "Iteration: 904/2657, with running loss: 3.098137617111206\n",
      "Iteration: 905/2657, with running loss: 3.1471798419952393\n",
      "Iteration: 906/2657, with running loss: 3.29365873336792\n",
      "Iteration: 907/2657, with running loss: 3.4399397373199463\n",
      "Iteration: 908/2657, with running loss: 2.5788378715515137\n",
      "Iteration: 909/2657, with running loss: 3.4122583866119385\n",
      "Iteration: 910/2657, with running loss: 3.721691846847534\n",
      "Iteration: 911/2657, with running loss: 3.278653621673584\n",
      "Iteration: 912/2657, with running loss: 2.289034843444824\n",
      "Iteration: 913/2657, with running loss: 3.4191417694091797\n",
      "Iteration: 914/2657, with running loss: 2.7177541255950928\n",
      "Iteration: 915/2657, with running loss: 2.833676815032959\n",
      "Iteration: 916/2657, with running loss: 3.34735369682312\n",
      "Iteration: 917/2657, with running loss: 2.758260488510132\n",
      "Iteration: 918/2657, with running loss: 2.838808059692383\n",
      "Iteration: 919/2657, with running loss: 3.086977243423462\n",
      "Iteration: 920/2657, with running loss: 3.3504300117492676\n",
      "Iteration: 921/2657, with running loss: 2.566653251647949\n",
      "Iteration: 922/2657, with running loss: 2.731569290161133\n",
      "Iteration: 923/2657, with running loss: 2.926913261413574\n",
      "Iteration: 924/2657, with running loss: 3.3390603065490723\n",
      "Iteration: 925/2657, with running loss: 3.6479592323303223\n",
      "Iteration: 926/2657, with running loss: 3.0266261100769043\n",
      "Iteration: 927/2657, with running loss: 3.442009210586548\n",
      "Iteration: 928/2657, with running loss: 3.5553529262542725\n",
      "Iteration: 929/2657, with running loss: 2.2568585872650146\n",
      "Iteration: 930/2657, with running loss: 3.232325792312622\n",
      "Iteration: 931/2657, with running loss: 2.9199705123901367\n",
      "Iteration: 932/2657, with running loss: 2.8470606803894043\n",
      "Iteration: 933/2657, with running loss: 3.648923397064209\n",
      "Iteration: 934/2657, with running loss: 2.5754618644714355\n",
      "Iteration: 935/2657, with running loss: 2.6403393745422363\n",
      "Iteration: 936/2657, with running loss: 2.5690996646881104\n",
      "Iteration: 937/2657, with running loss: 3.2033679485321045\n",
      "Iteration: 938/2657, with running loss: 3.390226125717163\n",
      "Iteration: 939/2657, with running loss: 2.1315691471099854\n",
      "Iteration: 940/2657, with running loss: 2.458536386489868\n",
      "Iteration: 941/2657, with running loss: 2.906207323074341\n",
      "Iteration: 942/2657, with running loss: 2.5654852390289307\n",
      "Iteration: 943/2657, with running loss: 2.5109705924987793\n",
      "Iteration: 944/2657, with running loss: 3.272808074951172\n",
      "Iteration: 945/2657, with running loss: 2.9121837615966797\n",
      "Iteration: 946/2657, with running loss: 3.254293918609619\n",
      "Iteration: 947/2657, with running loss: 2.4138827323913574\n",
      "Iteration: 948/2657, with running loss: 3.162393808364868\n",
      "Iteration: 949/2657, with running loss: 3.953808069229126\n",
      "Iteration: 950/2657, with running loss: 1.9270769357681274\n",
      "Iteration: 951/2657, with running loss: 2.7706518173217773\n",
      "Iteration: 952/2657, with running loss: 2.9310925006866455\n",
      "Iteration: 953/2657, with running loss: 3.2271814346313477\n",
      "Iteration: 954/2657, with running loss: 3.142688512802124\n",
      "Iteration: 955/2657, with running loss: 3.3134350776672363\n",
      "Iteration: 956/2657, with running loss: 2.976562738418579\n",
      "Iteration: 957/2657, with running loss: 3.116091012954712\n",
      "Iteration: 958/2657, with running loss: 2.4823644161224365\n",
      "Iteration: 959/2657, with running loss: 3.705761671066284\n",
      "Iteration: 960/2657, with running loss: 3.7430267333984375\n",
      "Iteration: 961/2657, with running loss: 3.0924057960510254\n",
      "Iteration: 962/2657, with running loss: 3.183954954147339\n",
      "Iteration: 963/2657, with running loss: 3.4937307834625244\n",
      "Iteration: 964/2657, with running loss: 2.742372989654541\n",
      "Iteration: 965/2657, with running loss: 2.9878759384155273\n",
      "Iteration: 966/2657, with running loss: 2.168931007385254\n",
      "Iteration: 967/2657, with running loss: 2.985208749771118\n",
      "Iteration: 968/2657, with running loss: 3.6616992950439453\n",
      "Iteration: 969/2657, with running loss: 1.9897111654281616\n",
      "Iteration: 970/2657, with running loss: 3.4250948429107666\n",
      "Iteration: 971/2657, with running loss: 2.850539445877075\n",
      "Iteration: 972/2657, with running loss: 2.3848800659179688\n",
      "Iteration: 973/2657, with running loss: 2.611121654510498\n",
      "Iteration: 974/2657, with running loss: 2.807978630065918\n",
      "Iteration: 975/2657, with running loss: 3.6604104042053223\n",
      "Iteration: 976/2657, with running loss: 2.7861251831054688\n",
      "Iteration: 977/2657, with running loss: 3.327925205230713\n",
      "Iteration: 978/2657, with running loss: 2.7019588947296143\n",
      "Iteration: 979/2657, with running loss: 2.6444718837738037\n",
      "Iteration: 980/2657, with running loss: 2.2253165245056152\n",
      "Iteration: 981/2657, with running loss: 3.769287109375\n",
      "Iteration: 982/2657, with running loss: 2.9228005409240723\n",
      "Iteration: 983/2657, with running loss: 2.9056448936462402\n",
      "Iteration: 984/2657, with running loss: 2.75886869430542\n",
      "Iteration: 985/2657, with running loss: 2.7381985187530518\n",
      "Iteration: 986/2657, with running loss: 2.9653737545013428\n",
      "Iteration: 987/2657, with running loss: 3.1014952659606934\n",
      "Iteration: 988/2657, with running loss: 2.9346065521240234\n",
      "Iteration: 989/2657, with running loss: 2.8323447704315186\n",
      "Iteration: 990/2657, with running loss: 1.6553486585617065\n",
      "Iteration: 991/2657, with running loss: 3.5873348712921143\n",
      "Iteration: 992/2657, with running loss: 3.1407506465911865\n",
      "Iteration: 993/2657, with running loss: 2.412921667098999\n",
      "Iteration: 994/2657, with running loss: 2.9550693035125732\n",
      "Iteration: 995/2657, with running loss: 3.167902946472168\n",
      "Iteration: 996/2657, with running loss: 3.4564805030822754\n",
      "Iteration: 997/2657, with running loss: 2.833730697631836\n",
      "Iteration: 998/2657, with running loss: 3.457921266555786\n",
      "Iteration: 999/2657, with running loss: 2.130617380142212\n",
      "Iteration: 1000/2657, with running loss: 2.56929612159729\n",
      "Iteration: 1001/2657, with running loss: 2.5521037578582764\n",
      "Iteration: 1002/2657, with running loss: 2.2843716144561768\n",
      "Iteration: 1003/2657, with running loss: 2.6633830070495605\n",
      "Iteration: 1004/2657, with running loss: 2.7803800106048584\n",
      "Iteration: 1005/2657, with running loss: 2.358212947845459\n",
      "Iteration: 1006/2657, with running loss: 3.369379758834839\n",
      "Iteration: 1007/2657, with running loss: 2.8337085247039795\n",
      "Iteration: 1008/2657, with running loss: 3.6823277473449707\n",
      "Iteration: 1009/2657, with running loss: 2.5600244998931885\n",
      "Iteration: 1010/2657, with running loss: 2.7361643314361572\n",
      "Iteration: 1011/2657, with running loss: 3.2167601585388184\n",
      "Iteration: 1012/2657, with running loss: 3.554295778274536\n",
      "Iteration: 1013/2657, with running loss: 3.6458899974823\n",
      "Iteration: 1014/2657, with running loss: 2.712489128112793\n",
      "Iteration: 1015/2657, with running loss: 2.9344732761383057\n",
      "Iteration: 1016/2657, with running loss: 3.2329020500183105\n",
      "Iteration: 1017/2657, with running loss: 2.818997383117676\n",
      "Iteration: 1018/2657, with running loss: 2.6850430965423584\n",
      "Iteration: 1019/2657, with running loss: 2.1162209510803223\n",
      "Iteration: 1020/2657, with running loss: 2.6520748138427734\n",
      "Iteration: 1021/2657, with running loss: 3.4999852180480957\n",
      "Iteration: 1022/2657, with running loss: 2.7890312671661377\n",
      "Iteration: 1023/2657, with running loss: 2.9241838455200195\n",
      "Iteration: 1024/2657, with running loss: 3.5150883197784424\n",
      "Iteration: 1025/2657, with running loss: 2.2298882007598877\n",
      "Iteration: 1026/2657, with running loss: 2.89538311958313\n",
      "Iteration: 1027/2657, with running loss: 2.538148880004883\n",
      "Iteration: 1028/2657, with running loss: 2.9044201374053955\n",
      "Iteration: 1029/2657, with running loss: 2.7276360988616943\n",
      "Iteration: 1030/2657, with running loss: 3.788344383239746\n",
      "Iteration: 1031/2657, with running loss: 3.2798407077789307\n",
      "Iteration: 1032/2657, with running loss: 3.4216690063476562\n",
      "Iteration: 1033/2657, with running loss: 3.231924295425415\n",
      "Iteration: 1034/2657, with running loss: 2.915128707885742\n",
      "Iteration: 1035/2657, with running loss: 2.1669936180114746\n",
      "Iteration: 1036/2657, with running loss: 3.1751832962036133\n",
      "Iteration: 1037/2657, with running loss: 2.0520999431610107\n",
      "Iteration: 1038/2657, with running loss: 3.1099464893341064\n",
      "Iteration: 1039/2657, with running loss: 2.487067699432373\n",
      "Iteration: 1040/2657, with running loss: 3.091949224472046\n",
      "Iteration: 1041/2657, with running loss: 2.401578903198242\n",
      "Iteration: 1042/2657, with running loss: 3.355064868927002\n",
      "Iteration: 1043/2657, with running loss: 2.8290154933929443\n",
      "Iteration: 1044/2657, with running loss: 3.5395236015319824\n",
      "Iteration: 1045/2657, with running loss: 2.9320106506347656\n",
      "Iteration: 1046/2657, with running loss: 3.3484127521514893\n",
      "Iteration: 1047/2657, with running loss: 3.3102195262908936\n",
      "Iteration: 1048/2657, with running loss: 3.4524128437042236\n",
      "Iteration: 1049/2657, with running loss: 1.9537829160690308\n",
      "Iteration: 1050/2657, with running loss: 2.840721845626831\n",
      "Iteration: 1051/2657, with running loss: 3.1612181663513184\n",
      "Iteration: 1052/2657, with running loss: 4.28821325302124\n",
      "Iteration: 1053/2657, with running loss: 2.244781494140625\n",
      "Iteration: 1054/2657, with running loss: 2.7720088958740234\n",
      "Iteration: 1055/2657, with running loss: 3.007888078689575\n",
      "Iteration: 1056/2657, with running loss: 3.2826032638549805\n",
      "Iteration: 1057/2657, with running loss: 2.5449841022491455\n",
      "Iteration: 1058/2657, with running loss: 3.3270673751831055\n",
      "Iteration: 1059/2657, with running loss: 3.5090925693511963\n",
      "Iteration: 1060/2657, with running loss: 2.2674245834350586\n",
      "Iteration: 1061/2657, with running loss: 2.0232672691345215\n",
      "Iteration: 1062/2657, with running loss: 3.2239091396331787\n",
      "Iteration: 1063/2657, with running loss: 2.7529592514038086\n",
      "Iteration: 1064/2657, with running loss: 3.237964391708374\n",
      "Iteration: 1065/2657, with running loss: 3.2543375492095947\n",
      "Iteration: 1066/2657, with running loss: 1.5850859880447388\n",
      "Iteration: 1067/2657, with running loss: 2.7215826511383057\n",
      "Iteration: 1068/2657, with running loss: 2.293523073196411\n",
      "Iteration: 1069/2657, with running loss: 3.391122579574585\n",
      "Iteration: 1070/2657, with running loss: 2.747575521469116\n",
      "Iteration: 1071/2657, with running loss: 2.7352423667907715\n",
      "Iteration: 1072/2657, with running loss: 3.191757917404175\n",
      "Iteration: 1073/2657, with running loss: 3.8610997200012207\n",
      "Iteration: 1074/2657, with running loss: 2.685473918914795\n",
      "Iteration: 1075/2657, with running loss: 2.8214542865753174\n",
      "Iteration: 1076/2657, with running loss: 3.119626522064209\n",
      "Iteration: 1077/2657, with running loss: 3.5097336769104004\n",
      "Iteration: 1078/2657, with running loss: 3.2290663719177246\n",
      "Iteration: 1079/2657, with running loss: 3.7210536003112793\n",
      "Iteration: 1080/2657, with running loss: 3.229748487472534\n",
      "Iteration: 1081/2657, with running loss: 3.010261297225952\n",
      "Iteration: 1082/2657, with running loss: 3.5597596168518066\n",
      "Iteration: 1083/2657, with running loss: 3.128735065460205\n",
      "Iteration: 1084/2657, with running loss: 3.434807300567627\n",
      "Iteration: 1085/2657, with running loss: 2.510737419128418\n",
      "Iteration: 1086/2657, with running loss: 3.000765800476074\n",
      "Iteration: 1087/2657, with running loss: 2.8159542083740234\n",
      "Iteration: 1088/2657, with running loss: 2.581880569458008\n",
      "Iteration: 1089/2657, with running loss: 3.4038186073303223\n",
      "Iteration: 1090/2657, with running loss: 2.7374753952026367\n",
      "Iteration: 1091/2657, with running loss: 3.0804502964019775\n",
      "Iteration: 1092/2657, with running loss: 2.41586971282959\n",
      "Iteration: 1093/2657, with running loss: 2.542015552520752\n",
      "Iteration: 1094/2657, with running loss: 2.8976049423217773\n",
      "Iteration: 1095/2657, with running loss: 3.061088800430298\n",
      "Iteration: 1096/2657, with running loss: 2.5276806354522705\n",
      "Iteration: 1097/2657, with running loss: 2.560345411300659\n",
      "Iteration: 1098/2657, with running loss: 3.2800536155700684\n",
      "Iteration: 1099/2657, with running loss: 3.139160633087158\n",
      "Iteration: 1100/2657, with running loss: 2.0688610076904297\n",
      "Iteration: 1101/2657, with running loss: 3.2417380809783936\n",
      "Iteration: 1102/2657, with running loss: 2.3556087017059326\n",
      "Iteration: 1103/2657, with running loss: 3.51412034034729\n",
      "Iteration: 1104/2657, with running loss: 2.041245222091675\n",
      "Iteration: 1105/2657, with running loss: 2.9337501525878906\n",
      "Iteration: 1106/2657, with running loss: 3.0856151580810547\n",
      "Iteration: 1107/2657, with running loss: 2.763840436935425\n",
      "Iteration: 1108/2657, with running loss: 3.0635976791381836\n",
      "Iteration: 1109/2657, with running loss: 3.6674957275390625\n",
      "Iteration: 1110/2657, with running loss: 2.518786668777466\n",
      "Iteration: 1111/2657, with running loss: 2.2603533267974854\n",
      "Iteration: 1112/2657, with running loss: 2.501116991043091\n",
      "Iteration: 1113/2657, with running loss: 2.405179977416992\n",
      "Iteration: 1114/2657, with running loss: 3.3153862953186035\n",
      "Iteration: 1115/2657, with running loss: 3.15584135055542\n",
      "Iteration: 1116/2657, with running loss: 3.331305503845215\n",
      "Iteration: 1117/2657, with running loss: 3.285045862197876\n",
      "Iteration: 1118/2657, with running loss: 2.794734477996826\n",
      "Iteration: 1119/2657, with running loss: 2.024099588394165\n",
      "Iteration: 1120/2657, with running loss: 2.9293081760406494\n",
      "Iteration: 1121/2657, with running loss: 2.873471975326538\n",
      "Iteration: 1122/2657, with running loss: 3.849069595336914\n",
      "Iteration: 1123/2657, with running loss: 3.469989776611328\n",
      "Iteration: 1124/2657, with running loss: 3.1930623054504395\n",
      "Iteration: 1125/2657, with running loss: 2.9432103633880615\n",
      "Iteration: 1126/2657, with running loss: 3.22672963142395\n",
      "Iteration: 1127/2657, with running loss: 2.8941612243652344\n",
      "Iteration: 1128/2657, with running loss: 2.436934232711792\n",
      "Iteration: 1129/2657, with running loss: 3.4291322231292725\n",
      "Iteration: 1130/2657, with running loss: 3.3311374187469482\n",
      "Iteration: 1131/2657, with running loss: 1.6052043437957764\n",
      "Iteration: 1132/2657, with running loss: 2.9978442192077637\n",
      "Iteration: 1133/2657, with running loss: 3.0032832622528076\n",
      "Iteration: 1134/2657, with running loss: 3.8656439781188965\n",
      "Iteration: 1135/2657, with running loss: 2.0010008811950684\n",
      "Iteration: 1136/2657, with running loss: 3.284405469894409\n",
      "Iteration: 1137/2657, with running loss: 3.820054531097412\n",
      "Iteration: 1138/2657, with running loss: 3.2731401920318604\n",
      "Iteration: 1139/2657, with running loss: 2.739466667175293\n",
      "Iteration: 1140/2657, with running loss: 2.6893904209136963\n",
      "Iteration: 1141/2657, with running loss: 3.022844076156616\n",
      "Iteration: 1142/2657, with running loss: 2.5029661655426025\n",
      "Iteration: 1143/2657, with running loss: 2.1501998901367188\n",
      "Iteration: 1144/2657, with running loss: 2.530637264251709\n",
      "Iteration: 1145/2657, with running loss: 2.7290682792663574\n",
      "Iteration: 1146/2657, with running loss: 3.489567756652832\n",
      "Iteration: 1147/2657, with running loss: 2.888044834136963\n",
      "Iteration: 1148/2657, with running loss: 3.0915987491607666\n",
      "Iteration: 1149/2657, with running loss: 3.0210962295532227\n",
      "Iteration: 1150/2657, with running loss: 2.688636541366577\n",
      "Iteration: 1151/2657, with running loss: 2.929760217666626\n",
      "Iteration: 1152/2657, with running loss: 3.0841875076293945\n",
      "Iteration: 1153/2657, with running loss: 3.062962055206299\n",
      "Iteration: 1154/2657, with running loss: 2.9026246070861816\n",
      "Iteration: 1155/2657, with running loss: 3.222820997238159\n",
      "Iteration: 1156/2657, with running loss: 2.728381395339966\n",
      "Iteration: 1157/2657, with running loss: 3.2986748218536377\n",
      "Iteration: 1158/2657, with running loss: 3.433260917663574\n",
      "Iteration: 1159/2657, with running loss: 2.8228533267974854\n",
      "Iteration: 1160/2657, with running loss: 3.2899768352508545\n",
      "Iteration: 1161/2657, with running loss: 3.0324649810791016\n",
      "Iteration: 1162/2657, with running loss: 2.706911325454712\n",
      "Iteration: 1163/2657, with running loss: 2.987612009048462\n",
      "Iteration: 1164/2657, with running loss: 2.7598462104797363\n",
      "Iteration: 1165/2657, with running loss: 2.8534538745880127\n",
      "Iteration: 1166/2657, with running loss: 3.8367533683776855\n",
      "Iteration: 1167/2657, with running loss: 3.1607377529144287\n",
      "Iteration: 1168/2657, with running loss: 3.1367578506469727\n",
      "Iteration: 1169/2657, with running loss: 2.9736969470977783\n",
      "Iteration: 1170/2657, with running loss: 3.530700445175171\n",
      "Iteration: 1171/2657, with running loss: 2.891831398010254\n",
      "Iteration: 1172/2657, with running loss: 2.367100715637207\n",
      "Iteration: 1173/2657, with running loss: 2.6310627460479736\n",
      "Iteration: 1174/2657, with running loss: 2.1246557235717773\n",
      "Iteration: 1175/2657, with running loss: 3.451425075531006\n",
      "Iteration: 1176/2657, with running loss: 3.2156665325164795\n",
      "Iteration: 1177/2657, with running loss: 3.069037437438965\n",
      "Iteration: 1178/2657, with running loss: 2.602328300476074\n",
      "Iteration: 1179/2657, with running loss: 2.880603313446045\n",
      "Iteration: 1180/2657, with running loss: 1.98729407787323\n",
      "Iteration: 1181/2657, with running loss: 3.0020604133605957\n",
      "Iteration: 1182/2657, with running loss: 2.5046205520629883\n",
      "Iteration: 1183/2657, with running loss: 3.324676036834717\n",
      "Iteration: 1184/2657, with running loss: 3.074491024017334\n",
      "Iteration: 1185/2657, with running loss: 2.8179163932800293\n",
      "Iteration: 1186/2657, with running loss: 2.8034591674804688\n",
      "Iteration: 1187/2657, with running loss: 2.8674049377441406\n",
      "Iteration: 1188/2657, with running loss: 3.4974658489227295\n",
      "Iteration: 1189/2657, with running loss: 3.558932304382324\n",
      "Iteration: 1190/2657, with running loss: 2.3280653953552246\n",
      "Iteration: 1191/2657, with running loss: 2.4667279720306396\n",
      "Iteration: 1192/2657, with running loss: 2.8022842407226562\n",
      "Iteration: 1193/2657, with running loss: 3.2150309085845947\n",
      "Iteration: 1194/2657, with running loss: 3.2053349018096924\n",
      "Iteration: 1195/2657, with running loss: 2.6130096912384033\n",
      "Iteration: 1196/2657, with running loss: 3.3657407760620117\n",
      "Iteration: 1197/2657, with running loss: 2.8573591709136963\n",
      "Iteration: 1198/2657, with running loss: 3.2632017135620117\n",
      "Iteration: 1199/2657, with running loss: 3.2557551860809326\n",
      "Iteration: 1200/2657, with running loss: 3.457719326019287\n",
      "Iteration: 1201/2657, with running loss: 2.936180830001831\n",
      "Iteration: 1202/2657, with running loss: 3.693964958190918\n",
      "Iteration: 1203/2657, with running loss: 3.30844783782959\n",
      "Iteration: 1204/2657, with running loss: 2.9989662170410156\n",
      "Iteration: 1205/2657, with running loss: 2.6596338748931885\n",
      "Iteration: 1206/2657, with running loss: 3.5841009616851807\n",
      "Iteration: 1207/2657, with running loss: 3.2831082344055176\n",
      "Iteration: 1208/2657, with running loss: 3.20485782623291\n",
      "Iteration: 1209/2657, with running loss: 3.465056896209717\n",
      "Iteration: 1210/2657, with running loss: 3.6648740768432617\n",
      "Iteration: 1211/2657, with running loss: 2.5370376110076904\n",
      "Iteration: 1212/2657, with running loss: 2.930598020553589\n",
      "Iteration: 1213/2657, with running loss: 2.2077529430389404\n",
      "Iteration: 1214/2657, with running loss: 2.268752098083496\n",
      "Iteration: 1215/2657, with running loss: 2.9906766414642334\n",
      "Iteration: 1216/2657, with running loss: 2.74151611328125\n",
      "Iteration: 1217/2657, with running loss: 3.671447992324829\n",
      "Iteration: 1218/2657, with running loss: 2.8233442306518555\n",
      "Iteration: 1219/2657, with running loss: 3.2680881023406982\n",
      "Iteration: 1220/2657, with running loss: 2.6148414611816406\n",
      "Iteration: 1221/2657, with running loss: 3.4472763538360596\n",
      "Iteration: 1222/2657, with running loss: 3.7225265502929688\n",
      "Iteration: 1223/2657, with running loss: 2.119894504547119\n",
      "Iteration: 1224/2657, with running loss: 2.7973501682281494\n",
      "Iteration: 1225/2657, with running loss: 3.079737424850464\n",
      "Iteration: 1226/2657, with running loss: 3.302219867706299\n",
      "Iteration: 1227/2657, with running loss: 3.525205135345459\n",
      "Iteration: 1228/2657, with running loss: 2.687283992767334\n",
      "Iteration: 1229/2657, with running loss: 2.1606743335723877\n",
      "Iteration: 1230/2657, with running loss: 2.854903221130371\n",
      "Iteration: 1231/2657, with running loss: 2.4501969814300537\n",
      "Iteration: 1232/2657, with running loss: 2.9845004081726074\n",
      "Iteration: 1233/2657, with running loss: 2.998065233230591\n",
      "Iteration: 1234/2657, with running loss: 3.0560595989227295\n",
      "Iteration: 1235/2657, with running loss: 2.5893094539642334\n",
      "Iteration: 1236/2657, with running loss: 3.2275381088256836\n",
      "Iteration: 1237/2657, with running loss: 2.5649218559265137\n",
      "Iteration: 1238/2657, with running loss: 3.0970113277435303\n",
      "Iteration: 1239/2657, with running loss: 2.3863346576690674\n",
      "Iteration: 1240/2657, with running loss: 2.058900833129883\n",
      "Iteration: 1241/2657, with running loss: 3.570528030395508\n",
      "Iteration: 1242/2657, with running loss: 3.437826633453369\n",
      "Iteration: 1243/2657, with running loss: 2.1875603199005127\n",
      "Iteration: 1244/2657, with running loss: 3.2434678077697754\n",
      "Iteration: 1245/2657, with running loss: 3.0209126472473145\n",
      "Iteration: 1246/2657, with running loss: 3.083693504333496\n",
      "Iteration: 1247/2657, with running loss: 2.677668333053589\n",
      "Iteration: 1248/2657, with running loss: 2.9173033237457275\n",
      "Iteration: 1249/2657, with running loss: 3.2485790252685547\n",
      "Iteration: 1250/2657, with running loss: 2.787741184234619\n",
      "Iteration: 1251/2657, with running loss: 3.1733148097991943\n",
      "Iteration: 1252/2657, with running loss: 3.232182025909424\n",
      "Iteration: 1253/2657, with running loss: 2.472114086151123\n",
      "Iteration: 1254/2657, with running loss: 2.923645257949829\n",
      "Iteration: 1255/2657, with running loss: 3.693580389022827\n",
      "Iteration: 1256/2657, with running loss: 2.8846144676208496\n",
      "Iteration: 1257/2657, with running loss: 2.5912156105041504\n",
      "Iteration: 1258/2657, with running loss: 1.8977571725845337\n",
      "Iteration: 1259/2657, with running loss: 2.940985679626465\n",
      "Iteration: 1260/2657, with running loss: 2.206712484359741\n",
      "Iteration: 1261/2657, with running loss: 2.217114210128784\n",
      "Iteration: 1262/2657, with running loss: 3.578972339630127\n",
      "Iteration: 1263/2657, with running loss: 2.9918065071105957\n",
      "Iteration: 1264/2657, with running loss: 3.514573574066162\n",
      "Iteration: 1265/2657, with running loss: 2.9978723526000977\n",
      "Iteration: 1266/2657, with running loss: 3.430863857269287\n",
      "Iteration: 1267/2657, with running loss: 3.45975923538208\n",
      "Iteration: 1268/2657, with running loss: 2.940279722213745\n",
      "Iteration: 1269/2657, with running loss: 2.522128105163574\n",
      "Iteration: 1270/2657, with running loss: 3.277570962905884\n",
      "Iteration: 1271/2657, with running loss: 2.6223816871643066\n",
      "Iteration: 1272/2657, with running loss: 3.004126787185669\n",
      "Iteration: 1273/2657, with running loss: 3.370011329650879\n",
      "Iteration: 1274/2657, with running loss: 3.432931423187256\n",
      "Iteration: 1275/2657, with running loss: 2.939344882965088\n",
      "Iteration: 1276/2657, with running loss: 3.7998545169830322\n",
      "Iteration: 1277/2657, with running loss: 3.3156630992889404\n",
      "Iteration: 1278/2657, with running loss: 2.782270669937134\n",
      "Iteration: 1279/2657, with running loss: 3.105123519897461\n",
      "Iteration: 1280/2657, with running loss: 3.361825704574585\n",
      "Iteration: 1281/2657, with running loss: 2.7889962196350098\n",
      "Iteration: 1282/2657, with running loss: 2.716546058654785\n",
      "Iteration: 1283/2657, with running loss: 2.998439073562622\n",
      "Iteration: 1284/2657, with running loss: 3.452552556991577\n",
      "Iteration: 1285/2657, with running loss: 3.049445867538452\n",
      "Iteration: 1286/2657, with running loss: 2.40690279006958\n",
      "Iteration: 1287/2657, with running loss: 3.3552842140197754\n",
      "Iteration: 1288/2657, with running loss: 4.000293731689453\n",
      "Iteration: 1289/2657, with running loss: 3.3898584842681885\n",
      "Iteration: 1290/2657, with running loss: 3.445568799972534\n",
      "Iteration: 1291/2657, with running loss: 2.913886547088623\n",
      "Iteration: 1292/2657, with running loss: 2.835005760192871\n",
      "Iteration: 1293/2657, with running loss: 3.2988393306732178\n",
      "Iteration: 1294/2657, with running loss: 2.8819692134857178\n",
      "Iteration: 1295/2657, with running loss: 2.323312520980835\n",
      "Iteration: 1296/2657, with running loss: 3.892911672592163\n",
      "Iteration: 1297/2657, with running loss: 2.735260486602783\n",
      "Iteration: 1298/2657, with running loss: 2.8998875617980957\n",
      "Iteration: 1299/2657, with running loss: 2.6296355724334717\n",
      "Iteration: 1300/2657, with running loss: 2.31758189201355\n",
      "Iteration: 1301/2657, with running loss: 3.6596806049346924\n",
      "Iteration: 1302/2657, with running loss: 2.8950023651123047\n",
      "Iteration: 1303/2657, with running loss: 3.9438796043395996\n",
      "Iteration: 1304/2657, with running loss: 3.0381088256835938\n",
      "Iteration: 1305/2657, with running loss: 2.732236385345459\n",
      "Iteration: 1306/2657, with running loss: 3.2339682579040527\n",
      "Iteration: 1307/2657, with running loss: 2.768096685409546\n",
      "Iteration: 1308/2657, with running loss: 2.574533462524414\n",
      "Iteration: 1309/2657, with running loss: 3.5037291049957275\n",
      "Iteration: 1310/2657, with running loss: 2.8627750873565674\n",
      "Iteration: 1311/2657, with running loss: 3.079129695892334\n",
      "Iteration: 1312/2657, with running loss: 2.8826112747192383\n",
      "Iteration: 1313/2657, with running loss: 2.672442674636841\n",
      "Iteration: 1314/2657, with running loss: 3.024777889251709\n",
      "Iteration: 1315/2657, with running loss: 3.3079872131347656\n",
      "Iteration: 1316/2657, with running loss: 2.8554434776306152\n",
      "Iteration: 1317/2657, with running loss: 2.329925298690796\n",
      "Iteration: 1318/2657, with running loss: 2.4680469036102295\n",
      "Iteration: 1319/2657, with running loss: 3.4149065017700195\n",
      "Iteration: 1320/2657, with running loss: 3.170253276824951\n",
      "Iteration: 1321/2657, with running loss: 3.262192487716675\n",
      "Iteration: 1322/2657, with running loss: 2.3533213138580322\n",
      "Iteration: 1323/2657, with running loss: 2.3588733673095703\n",
      "Iteration: 1324/2657, with running loss: 3.519120216369629\n",
      "Iteration: 1325/2657, with running loss: 2.268826484680176\n",
      "Iteration: 1326/2657, with running loss: 3.6609740257263184\n",
      "Iteration: 1327/2657, with running loss: 3.0367555618286133\n",
      "Iteration: 1328/2657, with running loss: 3.0483579635620117\n",
      "Iteration: 1329/2657, with running loss: 2.9744057655334473\n",
      "Iteration: 1330/2657, with running loss: 3.343266248703003\n",
      "Iteration: 1331/2657, with running loss: 3.3587989807128906\n",
      "Iteration: 1332/2657, with running loss: 2.8065240383148193\n",
      "Iteration: 1333/2657, with running loss: 3.197885751724243\n",
      "Iteration: 1334/2657, with running loss: 2.206589460372925\n",
      "Iteration: 1335/2657, with running loss: 2.1862001419067383\n",
      "Iteration: 1336/2657, with running loss: 2.4295363426208496\n",
      "Iteration: 1337/2657, with running loss: 2.080031394958496\n",
      "Iteration: 1338/2657, with running loss: 2.782146453857422\n",
      "Iteration: 1339/2657, with running loss: 2.933711290359497\n",
      "Iteration: 1340/2657, with running loss: 3.3212499618530273\n",
      "Iteration: 1341/2657, with running loss: 2.5428237915039062\n",
      "Iteration: 1342/2657, with running loss: 3.5119094848632812\n",
      "Iteration: 1343/2657, with running loss: 3.568955183029175\n",
      "Iteration: 1344/2657, with running loss: 2.6201584339141846\n",
      "Iteration: 1345/2657, with running loss: 2.3430840969085693\n",
      "Iteration: 1346/2657, with running loss: 2.9268500804901123\n",
      "Iteration: 1347/2657, with running loss: 3.0918991565704346\n",
      "Iteration: 1348/2657, with running loss: 2.1768198013305664\n",
      "Iteration: 1349/2657, with running loss: 2.982404947280884\n",
      "Iteration: 1350/2657, with running loss: 3.2331604957580566\n",
      "Iteration: 1351/2657, with running loss: 3.569437265396118\n",
      "Iteration: 1352/2657, with running loss: 2.9573402404785156\n",
      "Iteration: 1353/2657, with running loss: 3.03401780128479\n",
      "Iteration: 1354/2657, with running loss: 2.7664146423339844\n",
      "Iteration: 1355/2657, with running loss: 2.496753454208374\n",
      "Iteration: 1356/2657, with running loss: 4.012085914611816\n",
      "Iteration: 1357/2657, with running loss: 3.509934186935425\n",
      "Iteration: 1358/2657, with running loss: 3.3131725788116455\n",
      "Iteration: 1359/2657, with running loss: 2.6442272663116455\n",
      "Iteration: 1360/2657, with running loss: 3.2472994327545166\n",
      "Iteration: 1361/2657, with running loss: 2.1046793460845947\n",
      "Iteration: 1362/2657, with running loss: 2.3130576610565186\n",
      "Iteration: 1363/2657, with running loss: 2.665483236312866\n",
      "Iteration: 1364/2657, with running loss: 3.0786027908325195\n",
      "Iteration: 1365/2657, with running loss: 2.2178585529327393\n",
      "Iteration: 1366/2657, with running loss: 2.97751522064209\n",
      "Iteration: 1367/2657, with running loss: 3.2786219120025635\n",
      "Iteration: 1368/2657, with running loss: 3.3844006061553955\n",
      "Iteration: 1369/2657, with running loss: 3.0503978729248047\n",
      "Iteration: 1370/2657, with running loss: 3.5490450859069824\n",
      "Iteration: 1371/2657, with running loss: 2.7063159942626953\n",
      "Iteration: 1372/2657, with running loss: 3.337141513824463\n",
      "Iteration: 1373/2657, with running loss: 2.6664681434631348\n",
      "Iteration: 1374/2657, with running loss: 2.4769973754882812\n",
      "Iteration: 1375/2657, with running loss: 2.4399831295013428\n",
      "Iteration: 1376/2657, with running loss: 3.668692111968994\n",
      "Iteration: 1377/2657, with running loss: 2.1193041801452637\n",
      "Iteration: 1378/2657, with running loss: 3.153001546859741\n",
      "Iteration: 1379/2657, with running loss: 2.6928446292877197\n",
      "Iteration: 1380/2657, with running loss: 2.204787254333496\n",
      "Iteration: 1381/2657, with running loss: 3.2441775798797607\n",
      "Iteration: 1382/2657, with running loss: 2.8613526821136475\n",
      "Iteration: 1383/2657, with running loss: 2.9628000259399414\n",
      "Iteration: 1384/2657, with running loss: 3.264021396636963\n",
      "Iteration: 1385/2657, with running loss: 2.8068325519561768\n",
      "Iteration: 1386/2657, with running loss: 3.2367193698883057\n",
      "Iteration: 1387/2657, with running loss: 3.4126622676849365\n",
      "Iteration: 1388/2657, with running loss: 2.849808931350708\n",
      "Iteration: 1389/2657, with running loss: 3.23422908782959\n",
      "Iteration: 1390/2657, with running loss: 2.8165016174316406\n",
      "Iteration: 1391/2657, with running loss: 3.138476610183716\n",
      "Iteration: 1392/2657, with running loss: 3.231006622314453\n",
      "Iteration: 1393/2657, with running loss: 2.4098029136657715\n",
      "Iteration: 1394/2657, with running loss: 3.0449435710906982\n",
      "Iteration: 1395/2657, with running loss: 3.676126718521118\n",
      "Iteration: 1396/2657, with running loss: 3.2071077823638916\n",
      "Iteration: 1397/2657, with running loss: 2.9405550956726074\n",
      "Iteration: 1398/2657, with running loss: 3.0393757820129395\n",
      "Iteration: 1399/2657, with running loss: 2.7903034687042236\n",
      "Iteration: 1400/2657, with running loss: 1.8065186738967896\n",
      "Iteration: 1401/2657, with running loss: 3.359295606613159\n",
      "Iteration: 1402/2657, with running loss: 3.1685070991516113\n",
      "Iteration: 1403/2657, with running loss: 3.258610486984253\n",
      "Iteration: 1404/2657, with running loss: 2.974480390548706\n",
      "Iteration: 1405/2657, with running loss: 1.8637770414352417\n",
      "Iteration: 1406/2657, with running loss: 2.092133045196533\n",
      "Iteration: 1407/2657, with running loss: 2.7477030754089355\n",
      "Iteration: 1408/2657, with running loss: 2.335479259490967\n",
      "Iteration: 1409/2657, with running loss: 2.659320592880249\n",
      "Iteration: 1410/2657, with running loss: 2.9419047832489014\n",
      "Iteration: 1411/2657, with running loss: 2.5121240615844727\n",
      "Iteration: 1412/2657, with running loss: 3.1965646743774414\n",
      "Iteration: 1413/2657, with running loss: 3.044743776321411\n",
      "Iteration: 1414/2657, with running loss: 2.7441699504852295\n",
      "Iteration: 1415/2657, with running loss: 3.2932870388031006\n",
      "Iteration: 1416/2657, with running loss: 3.3121249675750732\n",
      "Iteration: 1417/2657, with running loss: 2.9024858474731445\n",
      "Iteration: 1418/2657, with running loss: 3.300062656402588\n",
      "Iteration: 1419/2657, with running loss: 3.9119832515716553\n",
      "Iteration: 1420/2657, with running loss: 3.6342217922210693\n",
      "Iteration: 1421/2657, with running loss: 2.982086658477783\n",
      "Iteration: 1422/2657, with running loss: 3.0178675651550293\n",
      "Iteration: 1423/2657, with running loss: 2.6696889400482178\n",
      "Iteration: 1424/2657, with running loss: 2.766026735305786\n",
      "Iteration: 1425/2657, with running loss: 2.2614998817443848\n",
      "Iteration: 1426/2657, with running loss: 2.9786489009857178\n",
      "Iteration: 1427/2657, with running loss: 2.6842920780181885\n",
      "Iteration: 1428/2657, with running loss: 2.6769566535949707\n",
      "Iteration: 1429/2657, with running loss: 2.6943187713623047\n",
      "Iteration: 1430/2657, with running loss: 2.2899420261383057\n",
      "Iteration: 1431/2657, with running loss: 3.6923608779907227\n",
      "Iteration: 1432/2657, with running loss: 2.3509304523468018\n",
      "Iteration: 1433/2657, with running loss: 2.9265382289886475\n",
      "Iteration: 1434/2657, with running loss: 2.7852413654327393\n",
      "Iteration: 1435/2657, with running loss: 2.6826674938201904\n",
      "Iteration: 1436/2657, with running loss: 2.8486576080322266\n",
      "Iteration: 1437/2657, with running loss: 3.023358106613159\n",
      "Iteration: 1438/2657, with running loss: 2.9514224529266357\n",
      "Iteration: 1439/2657, with running loss: 2.7098159790039062\n",
      "Iteration: 1440/2657, with running loss: 3.6774580478668213\n",
      "Iteration: 1441/2657, with running loss: 2.736363410949707\n",
      "Iteration: 1442/2657, with running loss: 3.364933490753174\n",
      "Iteration: 1443/2657, with running loss: 2.8476064205169678\n",
      "Iteration: 1444/2657, with running loss: 3.482370615005493\n",
      "Iteration: 1445/2657, with running loss: 2.7580549716949463\n",
      "Iteration: 1446/2657, with running loss: 3.8486218452453613\n",
      "Iteration: 1447/2657, with running loss: 2.7778031826019287\n",
      "Iteration: 1448/2657, with running loss: 3.794992208480835\n",
      "Iteration: 1449/2657, with running loss: 2.9155075550079346\n",
      "Iteration: 1450/2657, with running loss: 3.2806029319763184\n",
      "Iteration: 1451/2657, with running loss: 3.1766271591186523\n",
      "Iteration: 1452/2657, with running loss: 3.002797842025757\n",
      "Iteration: 1453/2657, with running loss: 3.950514078140259\n",
      "Iteration: 1454/2657, with running loss: 3.2586851119995117\n",
      "Iteration: 1455/2657, with running loss: 3.116351842880249\n",
      "Iteration: 1456/2657, with running loss: 3.2499077320098877\n",
      "Iteration: 1457/2657, with running loss: 2.958362102508545\n",
      "Iteration: 1458/2657, with running loss: 2.679291009902954\n",
      "Iteration: 1459/2657, with running loss: 3.4009933471679688\n",
      "Iteration: 1460/2657, with running loss: 3.273983955383301\n",
      "Iteration: 1461/2657, with running loss: 2.275182008743286\n",
      "Iteration: 1462/2657, with running loss: 3.108937978744507\n",
      "Iteration: 1463/2657, with running loss: 3.371988534927368\n",
      "Iteration: 1464/2657, with running loss: 3.25980544090271\n",
      "Iteration: 1465/2657, with running loss: 3.1677563190460205\n",
      "Iteration: 1466/2657, with running loss: 3.0694944858551025\n",
      "Iteration: 1467/2657, with running loss: 2.551408290863037\n",
      "Iteration: 1468/2657, with running loss: 2.7506728172302246\n",
      "Iteration: 1469/2657, with running loss: 2.7913222312927246\n",
      "Iteration: 1470/2657, with running loss: 3.169677734375\n",
      "Iteration: 1471/2657, with running loss: 3.967731237411499\n",
      "Iteration: 1472/2657, with running loss: 2.7851977348327637\n",
      "Iteration: 1473/2657, with running loss: 3.3374056816101074\n",
      "Iteration: 1474/2657, with running loss: 2.403884172439575\n",
      "Iteration: 1475/2657, with running loss: 2.3101842403411865\n",
      "Iteration: 1476/2657, with running loss: 3.5532639026641846\n",
      "Iteration: 1477/2657, with running loss: 2.3110690116882324\n",
      "Iteration: 1478/2657, with running loss: 3.468762159347534\n",
      "Iteration: 1479/2657, with running loss: 3.3568036556243896\n",
      "Iteration: 1480/2657, with running loss: 2.9924299716949463\n",
      "Iteration: 1481/2657, with running loss: 3.3658652305603027\n",
      "Iteration: 1482/2657, with running loss: 2.341824531555176\n",
      "Iteration: 1483/2657, with running loss: 3.04328989982605\n",
      "Iteration: 1484/2657, with running loss: 3.0696356296539307\n",
      "Iteration: 1485/2657, with running loss: 3.254598617553711\n",
      "Iteration: 1486/2657, with running loss: 3.1291487216949463\n",
      "Iteration: 1487/2657, with running loss: 3.191993236541748\n",
      "Iteration: 1488/2657, with running loss: 2.261613130569458\n",
      "Iteration: 1489/2657, with running loss: 3.2684478759765625\n",
      "Iteration: 1490/2657, with running loss: 2.709096908569336\n",
      "Iteration: 1491/2657, with running loss: 3.503594398498535\n",
      "Iteration: 1492/2657, with running loss: 3.2286484241485596\n",
      "Iteration: 1493/2657, with running loss: 3.2807257175445557\n",
      "Iteration: 1494/2657, with running loss: 3.382235527038574\n",
      "Iteration: 1495/2657, with running loss: 2.3737406730651855\n",
      "Iteration: 1496/2657, with running loss: 3.0930185317993164\n",
      "Iteration: 1497/2657, with running loss: 2.7643964290618896\n",
      "Iteration: 1498/2657, with running loss: 3.270873785018921\n",
      "Iteration: 1499/2657, with running loss: 1.7942543029785156\n",
      "Iteration: 1500/2657, with running loss: 3.1472995281219482\n",
      "Iteration: 1501/2657, with running loss: 2.461517095565796\n",
      "Iteration: 1502/2657, with running loss: 2.0294883251190186\n",
      "Iteration: 1503/2657, with running loss: 3.71830677986145\n",
      "Iteration: 1504/2657, with running loss: 3.4661343097686768\n",
      "Iteration: 1505/2657, with running loss: 2.61496639251709\n",
      "Iteration: 1506/2657, with running loss: 3.550025463104248\n",
      "Iteration: 1507/2657, with running loss: 3.0666162967681885\n",
      "Iteration: 1508/2657, with running loss: 3.7164971828460693\n",
      "Iteration: 1509/2657, with running loss: 2.626826047897339\n",
      "Iteration: 1510/2657, with running loss: 3.4004268646240234\n",
      "Iteration: 1511/2657, with running loss: 3.30086088180542\n",
      "Iteration: 1512/2657, with running loss: 3.0266168117523193\n",
      "Iteration: 1513/2657, with running loss: 3.4287171363830566\n",
      "Iteration: 1514/2657, with running loss: 3.3678884506225586\n",
      "Iteration: 1515/2657, with running loss: 3.1026411056518555\n",
      "Iteration: 1516/2657, with running loss: 2.537745714187622\n",
      "Iteration: 1517/2657, with running loss: 2.4151556491851807\n",
      "Iteration: 1518/2657, with running loss: 3.0239005088806152\n",
      "Iteration: 1519/2657, with running loss: 2.6257970333099365\n",
      "Iteration: 1520/2657, with running loss: 2.5575685501098633\n",
      "Iteration: 1521/2657, with running loss: 3.0952260494232178\n",
      "Iteration: 1522/2657, with running loss: 2.347188949584961\n",
      "Iteration: 1523/2657, with running loss: 3.4382195472717285\n",
      "Iteration: 1524/2657, with running loss: 2.343935012817383\n",
      "Iteration: 1525/2657, with running loss: 2.5232105255126953\n",
      "Iteration: 1526/2657, with running loss: 3.6695408821105957\n",
      "Iteration: 1527/2657, with running loss: 3.2888176441192627\n",
      "Iteration: 1528/2657, with running loss: 2.1567585468292236\n",
      "Iteration: 1529/2657, with running loss: 2.551016092300415\n",
      "Iteration: 1530/2657, with running loss: 2.8461854457855225\n",
      "Iteration: 1531/2657, with running loss: 2.5838990211486816\n",
      "Iteration: 1532/2657, with running loss: 3.1539695262908936\n",
      "Iteration: 1533/2657, with running loss: 2.818505048751831\n",
      "Iteration: 1534/2657, with running loss: 3.1604201793670654\n",
      "Iteration: 1535/2657, with running loss: 2.804489850997925\n",
      "Iteration: 1536/2657, with running loss: 3.22200083732605\n",
      "Iteration: 1537/2657, with running loss: 2.510305166244507\n",
      "Iteration: 1538/2657, with running loss: 3.0616366863250732\n",
      "Iteration: 1539/2657, with running loss: 3.4209606647491455\n",
      "Iteration: 1540/2657, with running loss: 3.4966018199920654\n",
      "Iteration: 1541/2657, with running loss: 2.7508785724639893\n",
      "Iteration: 1542/2657, with running loss: 2.5143020153045654\n",
      "Iteration: 1543/2657, with running loss: 2.733506917953491\n",
      "Iteration: 1544/2657, with running loss: 2.8849899768829346\n",
      "Iteration: 1545/2657, with running loss: 2.602893114089966\n",
      "Iteration: 1546/2657, with running loss: 2.636894702911377\n",
      "Iteration: 1547/2657, with running loss: 2.6540262699127197\n",
      "Iteration: 1548/2657, with running loss: 3.285447120666504\n",
      "Iteration: 1549/2657, with running loss: 2.4218268394470215\n",
      "Iteration: 1550/2657, with running loss: 3.076601266860962\n",
      "Iteration: 1551/2657, with running loss: 2.447648763656616\n",
      "Iteration: 1552/2657, with running loss: 3.6014785766601562\n",
      "Iteration: 1553/2657, with running loss: 3.721832752227783\n",
      "Iteration: 1554/2657, with running loss: 2.5736031532287598\n",
      "Iteration: 1555/2657, with running loss: 3.2074570655822754\n",
      "Iteration: 1556/2657, with running loss: 2.6822280883789062\n",
      "Iteration: 1557/2657, with running loss: 3.2689743041992188\n",
      "Iteration: 1558/2657, with running loss: 2.6349093914031982\n",
      "Iteration: 1559/2657, with running loss: 2.5572781562805176\n",
      "Iteration: 1560/2657, with running loss: 2.7504162788391113\n",
      "Iteration: 1561/2657, with running loss: 2.9067814350128174\n",
      "Iteration: 1562/2657, with running loss: 3.669638156890869\n",
      "Iteration: 1563/2657, with running loss: 3.0410594940185547\n",
      "Iteration: 1564/2657, with running loss: 3.0278327465057373\n",
      "Iteration: 1565/2657, with running loss: 3.355921983718872\n",
      "Iteration: 1566/2657, with running loss: 2.618415355682373\n",
      "Iteration: 1567/2657, with running loss: 2.243022918701172\n",
      "Iteration: 1568/2657, with running loss: 2.6422483921051025\n",
      "Iteration: 1569/2657, with running loss: 3.4100427627563477\n",
      "Iteration: 1570/2657, with running loss: 2.778137445449829\n",
      "Iteration: 1571/2657, with running loss: 2.8362128734588623\n",
      "Iteration: 1572/2657, with running loss: 3.634800672531128\n",
      "Iteration: 1573/2657, with running loss: 2.5904979705810547\n",
      "Iteration: 1574/2657, with running loss: 2.4583120346069336\n",
      "Iteration: 1575/2657, with running loss: 2.8954837322235107\n",
      "Iteration: 1576/2657, with running loss: 3.745673418045044\n",
      "Iteration: 1577/2657, with running loss: 2.8730697631835938\n",
      "Iteration: 1578/2657, with running loss: 2.750365972518921\n",
      "Iteration: 1579/2657, with running loss: 2.4598209857940674\n",
      "Iteration: 1580/2657, with running loss: 2.2584612369537354\n",
      "Iteration: 1581/2657, with running loss: 2.8716495037078857\n",
      "Iteration: 1582/2657, with running loss: 3.2212564945220947\n",
      "Iteration: 1583/2657, with running loss: 2.313664674758911\n",
      "Iteration: 1584/2657, with running loss: 2.85034441947937\n",
      "Iteration: 1585/2657, with running loss: 3.1600403785705566\n",
      "Iteration: 1586/2657, with running loss: 2.6387853622436523\n",
      "Iteration: 1587/2657, with running loss: 3.5209691524505615\n",
      "Iteration: 1588/2657, with running loss: 3.094529390335083\n",
      "Iteration: 1589/2657, with running loss: 2.5208728313446045\n",
      "Iteration: 1590/2657, with running loss: 3.4892616271972656\n",
      "Iteration: 1591/2657, with running loss: 3.2061562538146973\n",
      "Iteration: 1592/2657, with running loss: 2.5956473350524902\n",
      "Iteration: 1593/2657, with running loss: 2.8204472064971924\n",
      "Iteration: 1594/2657, with running loss: 3.083369016647339\n",
      "Iteration: 1595/2657, with running loss: 2.3042664527893066\n",
      "Iteration: 1596/2657, with running loss: 2.4037692546844482\n",
      "Iteration: 1597/2657, with running loss: 3.4129292964935303\n",
      "Iteration: 1598/2657, with running loss: 3.3859570026397705\n",
      "Iteration: 1599/2657, with running loss: 2.544792652130127\n",
      "Iteration: 1600/2657, with running loss: 2.86421537399292\n",
      "Iteration: 1601/2657, with running loss: 3.8081982135772705\n",
      "Iteration: 1602/2657, with running loss: 2.6900088787078857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17344\\3780170787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17344\\3780170787.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, opt, loss_fn, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Epoch {epoch + 1}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17344\\3879817903.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, opt, loss_fn, dataloader)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_loader1, val_loader1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "start_token_id = tokenizer.cls_token_id\n",
    "eos_token_id = tokenizer.sep_token_id\n",
    "\n",
    "def predict(model, input_sequence, SOS_token, EOS_token, max_length=150):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long)\n",
    "\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.get_tgt_mask(y_input.size(1))\n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]])\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tall, barren tree by a flowing creek. [SEP] A fallen tree trunk on a broken wood bridge. [SEP] The iron gate of a small palace with shrubs on the side. [SEP] Perpendicular plants are standing straight out of a muddy ground. [SEP] An outdoor shot showing a river up around leafless trees.\n",
      "\n",
      "The swamp had started as an upsetting stream of dead fishes, flies, and frogs. After having made it across the wobbly bridge. I finally got to the house just beyond the marsh. Even before that I walked through the suffocating swamp. I had to cross the creepy lake.\n",
      "\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 61])\n"
     ]
    }
   ],
   "source": [
    "new_entry = (['A tall, barren tree by a flowing creek.',\n",
    "   'A fallen tree trunk on a broken wood bridge.',\n",
    "   'The iron gate of a small palace with shrubs on the side.',\n",
    "   'Perpendicular plants are standing straight out of a muddy ground.',\n",
    "   'An outdoor shot showing a river up around leafless trees.'],\n",
    "  'The swamp had started as an upsetting stream of dead fishes, flies, and frogs. After having made it across the wobbly bridge. I finally got to the house just beyond the marsh. Even before that I walked through the suffocating swamp. I had to cross the creepy lake.')\n",
    "\n",
    "new_captions = new_entry[0]\n",
    "new_story = new_entry[1]\n",
    "input_text = ' [SEP] '.join(new_captions)\n",
    "print(input_text)\n",
    "print()\n",
    "print(new_story)\n",
    "caption_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "target_ids = tokenizer(new_story, return_tensors=\"pt\").input_ids\n",
    "print()\n",
    "print(caption_ids.shape)\n",
    "print(target_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 3193, 1997, 1996, 2103, 2001, 3376, 1012, 1996, 2103, 2001, 3376, 1012, 1996, 3193, 2001, 3376, 1012, 1996, 2103, 2001, 3376, 1012, 1996, 2103, 2001, 3376, 1012, 1996, 2103, 2001, 3376, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "generated_ids = predict(model, caption_ids, start_token_id, eos_token_id)\n",
    "print(generated_ids)\n",
    "story = tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the view of the city was beautiful. the city was beautiful. the view was beautiful. the city was beautiful. the city was beautiful. the city was beautiful.\n"
     ]
    }
   ],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
